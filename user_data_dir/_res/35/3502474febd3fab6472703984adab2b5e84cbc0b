{"title": "What is Big O Notation Explained: Space and Time Complexity", "byline": "Shen Huang", "dir": null, "lang": "en", "content": "<div class=\"page\" id=\"readability-page-1\"><div id=\"site-main\">\n<article><h1>What is Big O Notation Explained: Space and Time Complexity</h1>\n<section>\n<section data-test-label=\"post-content\">\n<p>Do you really understand Big O? If so, then this will refresh your understanding before an interview. If not, don\u2019t worry \u2014 come and join us for some endeavors in computer science.</p><p>If you have taken some algorithm related courses, you\u2019ve probably heard of the term <strong>Big O notation</strong>. If you haven\u2019t, we will go over it here, and then get a deeper understanding of what it really is.</p><p>Big O notation is one of the most fundamental tools for computer scientists to analyze the cost of an algorithm. It is a good practice for software engineers to understand in-depth as well. </p><p>This article is written with the assumption that you have already tackled some code. Also, some in-depth material also requires high-school math fundamentals, and therefore can be a bit less comfortable to total beginners. But if you are ready, let\u2019s get started!</p><p>In this article, we will have an in-depth discussion about Big O notation. We will start with an example algorithm to open up our understanding. Then, we will go into the mathematics a little bit to have a formal understanding. After that we will go over some common variations of Big O notation. In the end, we will discuss some of the limitations of Big O in a practical scenario. A table of contents can be found below.</p><h3 id=\"table-of-contents\">Table of Contents</h3><ol><li>What is Big O notation, and why does it matter</li><li>Formal Definition of Big O notation</li><li>Big O, Little O, Omega &amp; Theta</li><li>Complexity Comparison Between Typical Big Os</li><li>Time &amp; Space Complexity</li><li>Best, Average, Worst, Expected Complexity</li><li>Why Big O doesn\u2019t matter</li><li>In the end\u2026</li></ol><p>So let\u2019s get started.</p><h3 id=\"1-what-is-big-o-notation-and-why-does-it-matter\">1. What is Big O Notation, and why does it matter</h3><blockquote>\u201cBig O notation is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity. It is a member of a family of notations invented by Paul Bachmann, Edmund Landau, and others, collectively called Bachmann\u2013Landau notation or asymptotic notation.\u201d<p>\u2014 Wikipedia\u2019s definition of Big O notation</p></blockquote><p>In plain words, Big O notation describes the <strong>complexity</strong> of your code using algebraic terms.</p><p>To understand what Big O notation is, we can take a look at a typical example, <strong><em>O(n\u00b2)</em></strong>, which is usually pronounced <strong><em>\u201cBig O squared\u201d</em></strong>. The letter <strong><em>\u201cn\u201d</em></strong> here represents the <strong>input size</strong>, and the function <strong><em>\u201cg(n) = n\u00b2\u201d</em></strong> inside the <strong><em>\u201cO()\u201d</em></strong> gives us an idea of how complex the algorithm is with respect to the input size.</p><p>A typical algorithm that has the complexity of O(n\u00b2) would be the <strong>selection sort</strong> algorithm. Selection sort is a sorting algorithm that iterates through the list to ensure every element at index <strong><em>i</em></strong> is the <strong><em>ith</em></strong> smallest/largest element of the list. The <strong>CODEPEN</strong> below gives a visual example of it.</p><figure></figure><p>The algorithm can be described by the following code. In order to make sure the <em>ith</em> element is the <em>ith</em> smallest element in the list, this algorithm first iterates through the list with a for loop. Then for every element it uses another for loop to find the smallest element in the remaining part of the list.</p><pre tabindex=\"0\"><code>SelectionSort(List) {\n  for(i from 0 to List.Length) {\n    SmallestElement = List[i]\n    for(j from i to List.Length) {\n      if(SmallestElement &gt; List[j]) {\n        SmallestElement = List[j]\n      }\n    }\n    Swap(List[i], SmallestElement)\n  }\n}</code></pre><p>In this scenario, we consider the variable <strong><em>List</em></strong> as the input, thus input size n is the <strong><em>number of elements inside List</em></strong>. Assume the if statement, and the value assignment bounded by the if statement, takes constant time. Then we can find the big O notation for the SelectionSort function by analyzing how many times the statements are executed.</p><p>First the inner for loop runs the statements inside n times. And then after <strong><em>i</em></strong> is incremented, the inner for loop runs for n-1 times\u2026 \u2026until it runs once, then both of the for loops reach their terminating conditions.</p><figure><img alt=\"1_1ajbPJXjt3z7CofVODlaCw\" height=\"355\" loading=\"lazy\" sizes=\"(min-width: 720px) 720px\" src=\"https://www.freecodecamp.org/news/content/images/2021/06/1_1ajbPJXjt3z7CofVODlaCw.png\" srcset=\"https://www.freecodecamp.org/news/content/images/size/w600/2021/06/1_1ajbPJXjt3z7CofVODlaCw.png 600w, https://www.freecodecamp.org/news/content/images/2021/06/1_1ajbPJXjt3z7CofVODlaCw.png 800w\" width=\"800\"/><figcaption>Selection Sort Loops Illustrated</figcaption></figure><p>This actually ends up giving us a geometric sum, and with some <a href=\"https://en.wikipedia.org/wiki/1_%2B_2_%2B_3_%2B_4_%2B_%E2%8B%AF\" rel=\"noopener\">high-school math</a> we would find that the inner loop will repeat for 1+2 \u2026 + n times, which equals n(n-1)/2 times. If we multiply this out, we will end up getting n\u00b2/2-n/2.</p><p>When we calculate big O notation, we only care about the <strong>dominant terms</strong>, and we do not care about the coefficients. Thus we take the n\u00b2 as our final big O. We write it as O(n\u00b2), which again is pronounced <em>\u201cBig O squared\u201d</em>.</p><p>Now you may be wondering, what is this <strong><em>\u201cdominant term\u201d</em></strong> all about? And why do we not care about the coefficients? Don\u2019t worry, we will go over them one by one. It may be a little bit hard to understand at the beginning, but it will all make a lot more sense as you read through the next section.</p><h3 id=\"2-formal-definition-of-big-o-notation\">2. Formal Definition of Big O notation</h3><p>Once upon a time there was an Indian king who wanted to reward a wise man for his excellence. The wise man asked for nothing but some wheat that would fill up a chess board.</p><p>But here were his rules: in the first tile he wants 1 grain of wheat, then 2 on the second tile, then 4 on the next one\u2026each tile on the chess board needed to be filled by double the amount of grains as the previous one. The na\u00efve king agreed without hesitation, thinking it would be a trivial demand to fulfill, until he actually went on and tried it\u2026</p><figure><img alt=\"0_em0jJ2rgj-ZapCef\" height=\"594\" loading=\"lazy\" sizes=\"(min-width: 720px) 720px\" src=\"https://www.freecodecamp.org/news/content/images/2021/06/0_em0jJ2rgj-ZapCef.jpg\" srcset=\"https://www.freecodecamp.org/news/content/images/size/w600/2021/06/0_em0jJ2rgj-ZapCef.jpg 600w, https://www.freecodecamp.org/news/content/images/2021/06/0_em0jJ2rgj-ZapCef.jpg 800w\" width=\"800\"/><figcaption>Wheat and Chess Board, Image from <a href=\"https://en.wikipedia.org/wiki/Wheat_and_chessboard_problem\">Wikipedia</a></figcaption></figure><p>So how many grains of wheat does the king owe the wise man? We know that a chess board has 8 squares by 8 squares, which totals 64 tiles. So the last tile should have a total of \u00a0<strong>2\u2076\u00b3</strong> grains of wheat. If you do a calculation online, <strong>for the entire chessboard,</strong> you will end up getting <strong>1.8446744*10\u00b9\u2079</strong> \u2013 that is about 18 followed by 18 zeroes. </p><p>Assuming that each grain of wheat weights 0.01 grams, that gives us 184,467,440,737 tons of wheat. And 184 billion tons is quite a lot, isn\u2019t it?</p><p>The numbers grow quite fast later for exponential growth don\u2019t they? The same logic goes for computer algorithms. If the required efforts to accomplish a task grow exponentially with respect to the input size, it can end up becoming enormously large.</p><p>As we will see in a moment, the growth of 2\u207f is much faster than n\u00b2. Now, with n = 64, the square of 64 is 4096. If you add that number to 2\u2076\u2074, it will be lost outside the significant digits. </p><p>This is why, when we look at the growth rate, we only care about the dominant terms. And since we want to analyze the growth with respect to the input size, the coefficients which only multiply the number rather than growing with the input size do not contain useful information.</p><p>Below is the formal definition of Big O:</p><figure><img alt=\"0_cyqWw3UxODl-wqJi\" height=\"600\" loading=\"lazy\" sizes=\"(min-width: 720px) 720px\" src=\"https://www.freecodecamp.org/news/content/images/2021/06/0_cyqWw3UxODl-wqJi.jpg\" srcset=\"https://www.freecodecamp.org/news/content/images/size/w600/2021/06/0_cyqWw3UxODl-wqJi.jpg 600w, https://www.freecodecamp.org/news/content/images/2021/06/0_cyqWw3UxODl-wqJi.jpg 800w\" width=\"800\"/><figcaption><a href=\"https://slideplayer.com/slide/9739625/\" rel=\"noopener\">CSE 373 Slides</a> from University of Washington</figcaption></figure><p>The formal definition is useful when you need to perform a math proof. For example, the time complexity for selection sort can be defined by the function f(n) = n\u00b2/2-n/2 as we have discussed in the previous section.</p><p>If we allow our function g(n) to be n\u00b2, we can find a constant c = 1, and a N\u2080 = 0, and so long as N &gt; N\u2080, N\u00b2 will always be greater than N\u00b2/2-N/2. We can easily prove this by subtracting N\u00b2/2 from both functions, then we can easily see N\u00b2/2 &gt; -N/2 to be true when N &gt; 0. Therefore, we can come up with the conclusion that f(n) = O(n\u00b2), in the other selection<em> sort is \u201cbig O</em> squared\u201d.</p><p>You might have noticed a little trick here. That is, if you make g(n) grow super fast, way faster than anything, O(g(n)) will always be great enough. For example, for any polynomial function, you can always be right by saying that they are O(2\u207f) because 2\u207f will eventually outgrow any polynomials.</p><p>Mathematically, you are right, but generally when we talk about Big O, we want to know the <strong>tight bound</strong> of the function. You will understand this more as you read through the next section.</p><p>But before we go, let\u2019s test your understanding with the following question. The answer will be found in later sections so it won\u2019t be a throw away.</p><blockquote><strong>Question:</strong> An image is represented by a 2D array of pixels. If you use a nested for loop to iterate through every pixel (that is, you have a for loop going through all the columns, then another for loop inside to go through all the rows), what is the time complexity of the algorithm when the image is considered as the input?</blockquote><h3 id=\"3-big-o-little-o-omega-theta\">3. Big O, Little O, Omega &amp; Theta</h3><blockquote>Big O: \u201cf(n) is O(g(n))\u201d iff for some constants c and N\u2080, f(N) \u2264 cg(N) for all N &gt; N\u2080<p>Omega: \u201cf(n) is \u03a9(g(n))\u201d iff for some constants c and N\u2080, f(N) \u2265 cg(N) for all N &gt; N\u2080</p><p>Theta: \u201cf(n) is \u0398(g(n))\u201d iff f(n) is O(g(n)) and f(n) is \u03a9(g(n))</p><p>Little O: \u201cf(n) is o(g(n))\u201d iff f(n) is O(g(n)) and f(n) is not \u0398(g(n))</p><p>\u2014Formal Definition of Big O, Omega, Theta and Little O</p></blockquote><p>In plain words:</p><ul><li><strong>Big O (O())</strong> describes the <strong>upper bound</strong> of the complexity.</li><li><strong>Omega (\u03a9())</strong> describes the <strong>lower bound</strong> of the complexity.</li><li><strong>Theta (\u0398())</strong> describes the <strong>exact bound</strong> of the complexity.</li><li><strong>Little O (o())</strong> describes the <strong>upper bound excluding the exact bound</strong>.</li></ul><figure><img alt=\"1_O-dcXbYXojkAPEnDuVZMvA\" height=\"441\" loading=\"lazy\" src=\"https://www.freecodecamp.org/news/content/images/2021/06/1_O-dcXbYXojkAPEnDuVZMvA.png\" width=\"594\"/><figcaption>Relationships between Big O, Little O, Omega &amp; Theta Illustrated</figcaption></figure><p>For example, the function g(n) = n\u00b2 + 3n is O(n\u00b3), o(n\u2074), \u0398(n\u00b2) and \u03a9(n). But you would still be right if you say it is \u03a9(n\u00b2) or O(n\u00b2).</p><p>Generally, when we talk about Big O, what we actually meant is Theta. It is kind of meaningless when you give an upper bound that is way larger than the scope of the analysis. This would be similar to solving inequalities by putting \u221e on the larger side, which will almost always make you right.</p><p>But how do we determine which functions are more complex than others? In the next section you will be reading, we will learn that in detail.</p><h3 id=\"4-complexity-comparison-between-typical-big-os\">4. Complexity Comparison Between Typical Big Os</h3><p>When we are trying to figure out the Big O for a particular function g(n), we only care about the <strong>dominant term</strong> of the function. The dominant term is the term that grows the fastest.</p><p>For example, n\u00b2 grows faster than n, so if we have something like g(n) = n\u00b2 + 5n + 6, it will be big O(n\u00b2). If you have taken some calculus before, this is very similar to the shortcut of finding limits for fractional polynomials, where you only care about the dominant term for numerators and denominators in the end.</p><figure><img alt=\"0_MPwgKd4lgXACfuNt\" height=\"298\" loading=\"lazy\" src=\"https://www.freecodecamp.org/news/content/images/2021/06/0_MPwgKd4lgXACfuNt.png\" width=\"498\"/><figcaption>Another way to look at Big O, Image from <a href=\"https://stackoverflow.com/questions/1364444/difference-between-big-o-and-little-o-notation\" rel=\"noopener\">Stack Overflow</a></figcaption></figure><p>But which function grows faster than the others? There are actually quite a few rules.</p><figure><img alt=\"1_KfZYFUT2OKfjekJlCeYvuQ\" height=\"556\" loading=\"lazy\" sizes=\"(min-width: 720px) 720px\" src=\"https://www.freecodecamp.org/news/content/images/2021/06/1_KfZYFUT2OKfjekJlCeYvuQ.jpeg\" srcset=\"https://www.freecodecamp.org/news/content/images/size/w600/2021/06/1_KfZYFUT2OKfjekJlCeYvuQ.jpeg 600w, https://www.freecodecamp.org/news/content/images/2021/06/1_KfZYFUT2OKfjekJlCeYvuQ.jpeg 800w\" width=\"800\"/><figcaption>Complexity Growth Illustration from <a href=\"http://bigocheatsheet.com/\" rel=\"noopener\">Big O Cheatsheet</a></figcaption></figure><h4 id=\"1-o-1-has-the-least-complexity\">1. O(1) has the least complexity</h4><p>Often called <strong><em>\u201cconstant time\u201d</em></strong>, if you can create an algorithm to solve the problem in O(1), you are probably at your best. In some scenarios, the complexity may go beyond O(1), then we can analyze them by finding its O(1/g(n)) counterpart. For example, O(1/n) is more complex than O(1/n\u00b2).</p><h4 id=\"2-o-log-n-is-more-complex-than-o-1-but-less-complex-than-polynomials\">2. O(log(n)) is more complex than O(1), but less complex than polynomials</h4><p>As complexity is often related to divide and conquer algorithms, O(log(n)) is generally a good complexity you can reach for sorting algorithms. O(log(n)) is less complex than O(\u221an), because the square root function can be considered a polynomial, where the exponent is 0.5.</p><h4 id=\"3-complexity-of-polynomials-increases-as-the-exponent-increases\">3. Complexity of polynomials increases as the exponent increases</h4><p>For example, O(n\u2075) is more complex than O(n\u2074). Due to the simplicity of it, we actually went over quite many examples of polynomials in the previous sections.</p><h4 id=\"4-exponentials-have-greater-complexity-than-polynomials-as-long-as-the-coefficients-are-positive-multiples-of-n\">4. Exponentials have greater complexity than polynomials as long as the coefficients are positive multiples of n</h4><p>O(2\u207f) is more complex than O(n\u2079\u2079), but O(2\u207f) is actually less complex than O(1). We generally take 2 as base for exponentials and logarithms because things tends to be binary in Computer Science, but exponents can be changed by changing the coefficients. If not specified, the base for logarithms is assumed to be 2.</p><h4 id=\"5-factorials-have-greater-complexity-than-exponentials\">5. Factorials have greater complexity than exponentials</h4><p>If you are interested in the reasoning, look up the <a href=\"https://en.wikipedia.org/wiki/Gamma_function\" rel=\"noopener\"><strong>Gamma function</strong></a>, it is an <a href=\"https://en.wikipedia.org/wiki/Analytic_continuation\" rel=\"noopener\"><strong>analytic continuation</strong></a> of a factorial. A short proof is that both factorials and exponentials have the same number of multiplications, but the numbers that get multiplied grow for factorials, while remaining constant for exponentials.</p><h4 id=\"6-multiplying-terms\">6. Multiplying terms</h4><p>When multiplying, the complexity will be greater than the original, but no more than the equivalence of multiplying something that is more complex. For example, O(n * log(n)) is more complex than O(n) but less complex than O(n\u00b2), because O(n\u00b2) = O(n * n) and n is more complex than log(n).</p><p>To test your understanding, try ranking the following functions from the most complex to the least complex. The solutions with detailed explanations can be found in a later section as you read. Some of them are meant to be tricky and may require some deeper understanding of math. As you get to the solution, you will understand them more.</p><blockquote><strong>Question:</strong> Rank following functions from the most complex to the least complex.</blockquote><figure><img alt=\"1_69bzUpQxBwZFLBimaMe7kQ\" height=\"282\" loading=\"lazy\" sizes=\"(min-width: 720px) 720px\" src=\"https://www.freecodecamp.org/news/content/images/2021/06/1_69bzUpQxBwZFLBimaMe7kQ.png\" srcset=\"https://www.freecodecamp.org/news/content/images/size/w600/2021/06/1_69bzUpQxBwZFLBimaMe7kQ.png 600w, https://www.freecodecamp.org/news/content/images/2021/06/1_69bzUpQxBwZFLBimaMe7kQ.png 800w\" width=\"800\"/><figcaption>Examples taken from <a href=\"https://www.chegg.com/homework-help/questions-and-answers/problem-ask-refresh-knowledge-asymptotic-notations-rank-following-functions-order-growth-f-q23698273\" rel=\"noopener\">Textbook Problems</a></figcaption></figure><blockquote><strong>Solution to Section 2 Question:</strong><p>It was actually meant to be a trick question to test your understanding. The question tries to make you answer O(n\u00b2) because there is a nested for loop. However, n is supposed to be the input size. Since the image array is the input, and every pixel was iterated through only once, the answer is actually O(n). The next section will go over more examples like this one.</p></blockquote><h3 id=\"5-time-space-complexity\">5. Time &amp; Space Complexity</h3><p>So far, we have only been discussing the time complexity of the algorithms. That is, we only care about how much time it takes for the program to complete the task. What also matters is the space the program takes to complete the task. The space complexity is related to how much memory the program will use, and therefore is also an important factor to analyze.</p><p>The space complexity works similarly to time complexity. For example, selection sort has a space complexity of O(1), because it only stores one minimum value and its index for comparison, the maximum space used does not increase with the input size.</p><p>Some algorithms, such as bucket sort, have a space complexity of O(n), but are able to chop down the time complexity to O(1). Bucket sort sorts the array by creating a sorted list of all the possible elements in the array, then increments the count whenever the element is encountered. In the end the sorted array will be the sorted list elements repeated by their counts.</p><figure><img alt=\"1_GfLWx2TXS55unwqZ5-X26w\" height=\"502\" loading=\"lazy\" sizes=\"(min-width: 720px) 720px\" src=\"https://www.freecodecamp.org/news/content/images/2021/06/1_GfLWx2TXS55unwqZ5-X26w.png\" srcset=\"https://www.freecodecamp.org/news/content/images/size/w600/2021/06/1_GfLWx2TXS55unwqZ5-X26w.png 600w, https://www.freecodecamp.org/news/content/images/2021/06/1_GfLWx2TXS55unwqZ5-X26w.png 761w\" width=\"761\"/><figcaption>Bucket Sort Visualization</figcaption></figure><h3 id=\"6-best-average-worst-expected-complexity\">6. Best, Average, Worst, Expected Complexity</h3><p>The complexity can also be analyzed as best case, worst case, average case and expected case.</p><p>Let\u2019s take <strong>insertion sort,</strong> for example. Insertion sort iterates through all the elements in the list. If the element is smaller than its previous element, it inserts the element backwards until it is larger than the previous element.</p><figure><img alt=\"0*C9ork5K0ay7_CLBv\" height=\"180\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/0*C9ork5K0ay7_CLBv.gif\" width=\"300\"/><figcaption>Insertion Sort Illustrated, Image from <a href=\"https://en.wikipedia.org/wiki/Insertion_sort\" rel=\"noopener\" target=\"_blank\" title=\"\">Wikipedia</a></figcaption></figure><p>If the array is initially sorted, no swap will be made. The algorithm will just iterate through the array once, which results a time complexity of O(n). Therefore, we would say that the <strong>best-case</strong> time complexity of insertion sort is O(n). A complexity of O(n) is also often called <strong>linear complexity</strong>.</p><p>Sometimes an algorithm just has bad luck. Quick sort, for example, will have to go through the list in O(n) time if the elements are sorted in the opposite order, but on average it sorts the array in O(n * log(n)) time. Generally, when we evaluate time complexity of an algorithm, we look at their <strong>worst-case</strong> performance. More on that and quick sort will be discussed in the next section as you read.</p><p>The average case complexity describes the expected performance of the algorithm. Sometimes involves calculating the probability of each scenarios. It can get complicated to go into the details and therefore not discussed in this article. Below is a cheat-sheet on the time and space complexity of typical algorithms.</p><figure><img alt=\"0_XZsrnwao98R3dGTB\" height=\"563\" loading=\"lazy\" sizes=\"(min-width: 720px) 720px\" src=\"https://www.freecodecamp.org/news/content/images/2021/06/0_XZsrnwao98R3dGTB.png\" srcset=\"https://www.freecodecamp.org/news/content/images/size/w600/2021/06/0_XZsrnwao98R3dGTB.png 600w, https://www.freecodecamp.org/news/content/images/2021/06/0_XZsrnwao98R3dGTB.png 800w\" width=\"800\"/><figcaption><a href=\"http://bigocheatsheet.com/\" rel=\"noopener\">Big O Cheatsheet</a> for Common Algorithms</figcaption></figure><blockquote><strong>Solution to Section 4 Question:</strong></blockquote><p>By inspecting the functions, we should be able to immediately rank the following polynomials from most complex to least complex with rule 3. Where the square root of n is just n to the power of 0.5.</p><figure><img alt=\"1_RKlbisO36urUbi237TjyrQ\" height=\"91\" loading=\"lazy\" sizes=\"(min-width: 720px) 720px\" src=\"https://www.freecodecamp.org/news/content/images/2021/06/1_RKlbisO36urUbi237TjyrQ.png\" srcset=\"https://www.freecodecamp.org/news/content/images/size/w600/2021/06/1_RKlbisO36urUbi237TjyrQ.png 600w, https://www.freecodecamp.org/news/content/images/2021/06/1_RKlbisO36urUbi237TjyrQ.png 800w\" width=\"800\"/></figure><p>Then by applying rules 2 and 6, we will get the following. Base 3 log can be converted to base 2 with <strong><a href=\"https://www.purplemath.com/modules/logrules5.htm\">log base conversions</a></strong>. Base 3 log still grows a little bit slower then base 2 logs, and therefore gets ranked after.</p><figure><img alt=\"1_6R1jrWMGXpKxBqtEre9q8Q\" height=\"52\" loading=\"lazy\" sizes=\"(min-width: 720px) 720px\" src=\"https://www.freecodecamp.org/news/content/images/2021/06/1_6R1jrWMGXpKxBqtEre9q8Q.png\" srcset=\"https://www.freecodecamp.org/news/content/images/size/w600/2021/06/1_6R1jrWMGXpKxBqtEre9q8Q.png 600w, https://www.freecodecamp.org/news/content/images/2021/06/1_6R1jrWMGXpKxBqtEre9q8Q.png 800w\" width=\"800\"/></figure><p>The rest may look a little bit tricky, but let\u2019s try to unveil their true faces and see where we can put them.</p><p>First of all, 2 to the power of 2 to the power of n is greater than 2 to the power of n, and the +1 spices it up even more.</p><figure><img alt=\"1_eGLwpHDUJtr6CuALrpcQ2w\" height=\"220\" loading=\"lazy\" src=\"https://www.freecodecamp.org/news/content/images/2021/06/1_eGLwpHDUJtr6CuALrpcQ2w.png\" width=\"434\"/></figure><p>And then since we know 2 to the power of log(n) with based 2 is equal to n, we can convert the following. The log with 0.001 as exponent grows a little bit more than constants, but less than almost anything else.</p><figure><img alt=\"1_4yo7najRBY_OaTnDpT3cIg\" height=\"338\" loading=\"lazy\" sizes=\"(min-width: 720px) 720px\" src=\"https://www.freecodecamp.org/news/content/images/2021/06/1_4yo7najRBY_OaTnDpT3cIg.png\" srcset=\"https://www.freecodecamp.org/news/content/images/size/w600/2021/06/1_4yo7najRBY_OaTnDpT3cIg.png 600w, https://www.freecodecamp.org/news/content/images/2021/06/1_4yo7najRBY_OaTnDpT3cIg.png 800w\" width=\"800\"/></figure><p>The one with n to the power of log(log(n)) is actually a variation of the <a href=\"https://en.wikipedia.org/wiki/Time_complexity#Quasi-polynomial_time\" rel=\"noopener\"><strong>quasi-polynomial</strong></a>, which is greater than polynomial but less than exponential. Since log(n) grows slower than n, the complexity of it is a bit less. The one with the inverse log converges to constant, as 1/log(n) diverges to infinity.</p><figure><img alt=\"1_ZYUCFuiSbOibqdSfmuwdvA\" height=\"148\" loading=\"lazy\" src=\"https://www.freecodecamp.org/news/content/images/2021/06/1_ZYUCFuiSbOibqdSfmuwdvA.png\" width=\"434\"/></figure><p>The factorials can be represented by multiplications, and thus can be converted to additions outside the logarithmic function. The \u201cn choose 2\u201d can be converted into a polynomial with a cubic term being the largest.</p><figure><img alt=\"1_cbrjlMGsWYCs36u831pLTA\" height=\"242\" loading=\"lazy\" sizes=\"(min-width: 720px) 720px\" src=\"https://www.freecodecamp.org/news/content/images/2021/06/1_cbrjlMGsWYCs36u831pLTA.png\" srcset=\"https://www.freecodecamp.org/news/content/images/size/w600/2021/06/1_cbrjlMGsWYCs36u831pLTA.png 600w, https://www.freecodecamp.org/news/content/images/2021/06/1_cbrjlMGsWYCs36u831pLTA.png 800w\" width=\"800\"/></figure><p>And finally, we can rank the functions from the most complex to the least complex.</p><figure><img alt=\"1_NHVggTVMGjGOe7SxtSgIpQ\" height=\"1096\" loading=\"lazy\" src=\"https://www.freecodecamp.org/news/content/images/2021/06/1_NHVggTVMGjGOe7SxtSgIpQ.png\" srcset=\"https://www.freecodecamp.org/news/content/images/size/w600/2021/06/1_NHVggTVMGjGOe7SxtSgIpQ.png 600w, https://www.freecodecamp.org/news/content/images/2021/06/1_NHVggTVMGjGOe7SxtSgIpQ.png 704w\" width=\"704\"/></figure><h3 id=\"why-bigo-doesn-t-matter\">Why BigO doesn\u2019t matter</h3><blockquote><strong>!!! \u2014 WARNING \u2014 !!!</strong><p>Contents discussed here are generally <strong>not accepted</strong> by most programmers in the world. Discuss it <strong>at your own risk</strong> in an interview. People actually blogged about how they <strong>failed</strong> their Google interviews because they questioned the authority, like here.</p><p><strong>!!! \u2014 WARNING \u2014 !!!</strong></p></blockquote><p>Since we have previously learned that the worst case time complexity for quick sort is O(n\u00b2), but O(n * log(n)) for merge sort, merge sort should be faster \u2014 right? Well you probably have guessed that the answer is false. The algorithms are just wired up in a way that makes quick sort the <em>\u201cquick sort\u201d</em>.</p><p>To demonstrate, check out this <a href=\"https://trinket.io/python/87a3166026\" rel=\"noopener\">trinket.io</a> I made. It compares the time for quick sort and merge sort. I have only managed to test it on arrays with a length up to 10000, but as you can see so far, the time for merge sort grows faster than quick sort. Despite quick sort having a worse case complexity of O(n\u00b2), the likelihood of that is really low. When it comes to the increase in speed quick sort has over merge sort bounded by the O(n * log(n)) complexity, quick sort ends up with a better performance in average.</p><figure><img alt=\"1_UvDTlLjNnQurODtnCWjEJg\" height=\"579\" loading=\"lazy\" sizes=\"(min-width: 720px) 720px\" src=\"https://www.freecodecamp.org/news/content/images/2021/06/1_UvDTlLjNnQurODtnCWjEJg.png\" srcset=\"https://www.freecodecamp.org/news/content/images/size/w600/2021/06/1_UvDTlLjNnQurODtnCWjEJg.png 600w, https://www.freecodecamp.org/news/content/images/2021/06/1_UvDTlLjNnQurODtnCWjEJg.png 800w\" width=\"800\"/><figcaption>Time Comparison between Quick Sort &amp; Merge Sort</figcaption></figure><p>I have also made the below graph to compare the ratio between the time they take, as it is hard to see them at lower values. And as you can see, the percentage time taken for quick sort is in a descending order.</p><figure><img alt=\"1_Zdm_8c-uU5941r7zJd4FPQ\" height=\"600\" loading=\"lazy\" sizes=\"(min-width: 720px) 720px\" src=\"https://www.freecodecamp.org/news/content/images/2021/06/1_Zdm_8c-uU5941r7zJd4FPQ.png\" srcset=\"https://www.freecodecamp.org/news/content/images/size/w600/2021/06/1_Zdm_8c-uU5941r7zJd4FPQ.png 600w, https://www.freecodecamp.org/news/content/images/2021/06/1_Zdm_8c-uU5941r7zJd4FPQ.png 800w\" width=\"800\"/><figcaption>Time Ratio between Quick Sort &amp; Merge Sort</figcaption></figure><p>The moral of the story is, Big O notation is only a mathematical analysis to provide a reference on the resources consumed by the algorithm. Practically, the results may be different. But it is generally a good practice trying to chop down the complexity of our algorithms, until we run into a case where we know what we are doing.</p><h3 id=\"in-the-end-\">In the end\u2026</h3><p>I like coding, learning new things and sharing them with the community. If there is anything in which you are particularly interested, please let me know. I generally write on web design, software architecture, mathematics and data science. You can find some great articles I have written before if you are interested in any of the topics above.</p><p>Hope you have a great time learning computer science!!!</p>\n</section>\n<hr/>\n<hr/>\n<p>\n        Learn to code for free. freeCodeCamp's open source curriculum has helped more than 40,000 people get jobs as developers. <a href=\"https://www.freecodecamp.org/learn/\" id=\"learn-to-code-cta\" rel=\"noopener noreferrer\" target=\"_blank\">Get started</a>\n</p>\n</section>\n</article>\n</div></div>", "textContent": "\n            \n                \n                \n                \n                    \n                \n                \n                    \n                            \nDo you really understand Big O? If so, then this will refresh your understanding before an interview. If not, don\u2019t worry \u2014 come and join us for some endeavors in computer science.If you have taken some algorithm related courses, you\u2019ve probably heard of the term Big O notation. If you haven\u2019t, we will go over it here, and then get a deeper understanding of what it really is.Big O notation is one of the most fundamental tools for computer scientists to analyze the cost of an algorithm. It is a good practice for software engineers to understand in-depth as well. This article is written with the assumption that you have already tackled some code. Also, some in-depth material also requires high-school math fundamentals, and therefore can be a bit less comfortable to total beginners. But if you are ready, let\u2019s get started!In this article, we will have an in-depth discussion about Big O notation. We will start with an example algorithm to open up our understanding. Then, we will go into the mathematics a little bit to have a formal understanding. After that we will go over some common variations of Big O notation. In the end, we will discuss some of the limitations of Big O in a practical scenario. A table of contents can be found below.Table of ContentsWhat is Big O notation, and why does it matterFormal Definition of Big O notationBig O, Little O, Omega & ThetaComplexity Comparison Between Typical Big OsTime & Space ComplexityBest, Average, Worst, Expected ComplexityWhy Big O doesn\u2019t matterIn the end\u2026So let\u2019s get started.1. What is Big O Notation, and why does it matter\u201cBig O notation is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity. It is a member of a family of notations invented by Paul Bachmann, Edmund Landau, and others, collectively called Bachmann\u2013Landau notation or asymptotic notation.\u201d\u2014 Wikipedia\u2019s definition of Big O notationIn plain words, Big O notation describes the complexity of your code using algebraic terms.To understand what Big O notation is, we can take a look at a typical example, O(n\u00b2), which is usually pronounced \u201cBig O squared\u201d. The letter \u201cn\u201d here represents the input size, and the function \u201cg(n) = n\u00b2\u201d inside the \u201cO()\u201d gives us an idea of how complex the algorithm is with respect to the input size.A typical algorithm that has the complexity of O(n\u00b2) would be the selection sort algorithm. Selection sort is a sorting algorithm that iterates through the list to ensure every element at index i is the ith smallest/largest element of the list. The CODEPEN below gives a visual example of it.The algorithm can be described by the following code. In order to make sure the ith element is the ith smallest element in the list, this algorithm first iterates through the list with a for loop. Then for every element it uses another for loop to find the smallest element in the remaining part of the list.SelectionSort(List) {\n  for(i from 0 to List.Length) {\n    SmallestElement = List[i]\n    for(j from i to List.Length) {\n      if(SmallestElement > List[j]) {\n        SmallestElement = List[j]\n      }\n    }\n    Swap(List[i], SmallestElement)\n  }\n}In this scenario, we consider the variable List as the input, thus input size n is the number of elements inside List. Assume the if statement, and the value assignment bounded by the if statement, takes constant time. Then we can find the big O notation for the SelectionSort function by analyzing how many times the statements are executed.First the inner for loop runs the statements inside n times. And then after i is incremented, the inner for loop runs for n-1 times\u2026 \u2026until it runs once, then both of the for loops reach their terminating conditions.Selection Sort Loops IllustratedThis actually ends up giving us a geometric sum, and with some high-school math we would find that the inner loop will repeat for 1+2 \u2026 + n times, which equals n(n-1)/2 times. If we multiply this out, we will end up getting n\u00b2/2-n/2.When we calculate big O notation, we only care about the dominant terms, and we do not care about the coefficients. Thus we take the n\u00b2 as our final big O. We write it as O(n\u00b2), which again is pronounced \u201cBig O squared\u201d.Now you may be wondering, what is this \u201cdominant term\u201d all about? And why do we not care about the coefficients? Don\u2019t worry, we will go over them one by one. It may be a little bit hard to understand at the beginning, but it will all make a lot more sense as you read through the next section.2. Formal Definition of Big O notationOnce upon a time there was an Indian king who wanted to reward a wise man for his excellence. The wise man asked for nothing but some wheat that would fill up a chess board.But here were his rules: in the first tile he wants 1 grain of wheat, then 2 on the second tile, then 4 on the next one\u2026each tile on the chess board needed to be filled by double the amount of grains as the previous one. The na\u00efve king agreed without hesitation, thinking it would be a trivial demand to fulfill, until he actually went on and tried it\u2026Wheat and Chess Board, Image from WikipediaSo how many grains of wheat does the king owe the wise man? We know that a chess board has 8 squares by 8 squares, which totals 64 tiles. So the last tile should have a total of \u00a02\u2076\u00b3 grains of wheat. If you do a calculation online, for the entire chessboard, you will end up getting 1.8446744*10\u00b9\u2079 \u2013 that is about 18 followed by 18 zeroes. Assuming that each grain of wheat weights 0.01 grams, that gives us 184,467,440,737 tons of wheat. And 184 billion tons is quite a lot, isn\u2019t it?The numbers grow quite fast later for exponential growth don\u2019t they? The same logic goes for computer algorithms. If the required efforts to accomplish a task grow exponentially with respect to the input size, it can end up becoming enormously large.As we will see in a moment, the growth of 2\u207f is much faster than n\u00b2. Now, with n = 64, the square of 64 is 4096. If you add that number to 2\u2076\u2074, it will be lost outside the significant digits. This is why, when we look at the growth rate, we only care about the dominant terms. And since we want to analyze the growth with respect to the input size, the coefficients which only multiply the number rather than growing with the input size do not contain useful information.Below is the formal definition of Big O:CSE 373 Slides from University of WashingtonThe formal definition is useful when you need to perform a math proof. For example, the time complexity for selection sort can be defined by the function f(n) = n\u00b2/2-n/2 as we have discussed in the previous section.If we allow our function g(n) to be n\u00b2, we can find a constant c = 1, and a N\u2080 = 0, and so long as N > N\u2080, N\u00b2 will always be greater than N\u00b2/2-N/2. We can easily prove this by subtracting N\u00b2/2 from both functions, then we can easily see N\u00b2/2 > -N/2 to be true when N > 0. Therefore, we can come up with the conclusion that f(n) = O(n\u00b2), in the other selection sort is \u201cbig O squared\u201d.You might have noticed a little trick here. That is, if you make g(n) grow super fast, way faster than anything, O(g(n)) will always be great enough. For example, for any polynomial function, you can always be right by saying that they are O(2\u207f) because 2\u207f will eventually outgrow any polynomials.Mathematically, you are right, but generally when we talk about Big O, we want to know the tight bound of the function. You will understand this more as you read through the next section.But before we go, let\u2019s test your understanding with the following question. The answer will be found in later sections so it won\u2019t be a throw away.Question: An image is represented by a 2D array of pixels. If you use a nested for loop to iterate through every pixel (that is, you have a for loop going through all the columns, then another for loop inside to go through all the rows), what is the time complexity of the algorithm when the image is considered as the input?3. Big O, Little O, Omega & ThetaBig O: \u201cf(n) is O(g(n))\u201d iff for some constants c and N\u2080, f(N) \u2264 cg(N) for all N > N\u2080Omega: \u201cf(n) is \u03a9(g(n))\u201d iff for some constants c and N\u2080, f(N) \u2265 cg(N) for all N > N\u2080Theta: \u201cf(n) is \u0398(g(n))\u201d iff f(n) is O(g(n)) and f(n) is \u03a9(g(n))Little O: \u201cf(n) is o(g(n))\u201d iff f(n) is O(g(n)) and f(n) is not \u0398(g(n))\u2014Formal Definition of Big O, Omega, Theta and Little OIn plain words:Big O (O()) describes the upper bound of the complexity.Omega (\u03a9()) describes the lower bound of the complexity.Theta (\u0398()) describes the exact bound of the complexity.Little O (o()) describes the upper bound excluding the exact bound.Relationships between Big O, Little O, Omega & Theta IllustratedFor example, the function g(n) = n\u00b2 + 3n is O(n\u00b3), o(n\u2074), \u0398(n\u00b2) and \u03a9(n). But you would still be right if you say it is \u03a9(n\u00b2) or O(n\u00b2).Generally, when we talk about Big O, what we actually meant is Theta. It is kind of meaningless when you give an upper bound that is way larger than the scope of the analysis. This would be similar to solving inequalities by putting \u221e on the larger side, which will almost always make you right.But how do we determine which functions are more complex than others? In the next section you will be reading, we will learn that in detail.4. Complexity Comparison Between Typical Big OsWhen we are trying to figure out the Big O for a particular function g(n), we only care about the dominant term of the function. The dominant term is the term that grows the fastest.For example, n\u00b2 grows faster than n, so if we have something like g(n) = n\u00b2 + 5n + 6, it will be big O(n\u00b2). If you have taken some calculus before, this is very similar to the shortcut of finding limits for fractional polynomials, where you only care about the dominant term for numerators and denominators in the end.Another way to look at Big O, Image from Stack OverflowBut which function grows faster than the others? There are actually quite a few rules.Complexity Growth Illustration from Big O Cheatsheet1. O(1) has the least complexityOften called \u201cconstant time\u201d, if you can create an algorithm to solve the problem in O(1), you are probably at your best. In some scenarios, the complexity may go beyond O(1), then we can analyze them by finding its O(1/g(n)) counterpart. For example, O(1/n) is more complex than O(1/n\u00b2).2. O(log(n)) is more complex than O(1), but less complex than polynomialsAs complexity is often related to divide and conquer algorithms, O(log(n)) is generally a good complexity you can reach for sorting algorithms. O(log(n)) is less complex than O(\u221an), because the square root function can be considered a polynomial, where the exponent is 0.5.3. Complexity of polynomials increases as the exponent increasesFor example, O(n\u2075) is more complex than O(n\u2074). Due to the simplicity of it, we actually went over quite many examples of polynomials in the previous sections.4. Exponentials have greater complexity than polynomials as long as the coefficients are positive multiples of nO(2\u207f) is more complex than O(n\u2079\u2079), but O(2\u207f) is actually less complex than O(1). We generally take 2 as base for exponentials and logarithms because things tends to be binary in Computer Science, but exponents can be changed by changing the coefficients. If not specified, the base for logarithms is assumed to be 2.5. Factorials have greater complexity than exponentialsIf you are interested in the reasoning, look up the Gamma function, it is an analytic continuation of a factorial. A short proof is that both factorials and exponentials have the same number of multiplications, but the numbers that get multiplied grow for factorials, while remaining constant for exponentials.6. Multiplying termsWhen multiplying, the complexity will be greater than the original, but no more than the equivalence of multiplying something that is more complex. For example, O(n * log(n)) is more complex than O(n) but less complex than O(n\u00b2), because O(n\u00b2) = O(n * n) and n is more complex than log(n).To test your understanding, try ranking the following functions from the most complex to the least complex. The solutions with detailed explanations can be found in a later section as you read. Some of them are meant to be tricky and may require some deeper understanding of math. As you get to the solution, you will understand them more.Question: Rank following functions from the most complex to the least complex.Examples taken from Textbook ProblemsSolution to Section 2 Question:It was actually meant to be a trick question to test your understanding. The question tries to make you answer O(n\u00b2) because there is a nested for loop. However, n is supposed to be the input size. Since the image array is the input, and every pixel was iterated through only once, the answer is actually O(n). The next section will go over more examples like this one.5. Time & Space ComplexitySo far, we have only been discussing the time complexity of the algorithms. That is, we only care about how much time it takes for the program to complete the task. What also matters is the space the program takes to complete the task. The space complexity is related to how much memory the program will use, and therefore is also an important factor to analyze.The space complexity works similarly to time complexity. For example, selection sort has a space complexity of O(1), because it only stores one minimum value and its index for comparison, the maximum space used does not increase with the input size.Some algorithms, such as bucket sort, have a space complexity of O(n), but are able to chop down the time complexity to O(1). Bucket sort sorts the array by creating a sorted list of all the possible elements in the array, then increments the count whenever the element is encountered. In the end the sorted array will be the sorted list elements repeated by their counts.Bucket Sort Visualization6. Best, Average, Worst, Expected ComplexityThe complexity can also be analyzed as best case, worst case, average case and expected case.Let\u2019s take insertion sort, for example. Insertion sort iterates through all the elements in the list. If the element is smaller than its previous element, it inserts the element backwards until it is larger than the previous element.Insertion Sort Illustrated, Image from WikipediaIf the array is initially sorted, no swap will be made. The algorithm will just iterate through the array once, which results a time complexity of O(n). Therefore, we would say that the best-case time complexity of insertion sort is O(n). A complexity of O(n) is also often called linear complexity.Sometimes an algorithm just has bad luck. Quick sort, for example, will have to go through the list in O(n) time if the elements are sorted in the opposite order, but on average it sorts the array in O(n * log(n)) time. Generally, when we evaluate time complexity of an algorithm, we look at their worst-case performance. More on that and quick sort will be discussed in the next section as you read.The average case complexity describes the expected performance of the algorithm. Sometimes involves calculating the probability of each scenarios. It can get complicated to go into the details and therefore not discussed in this article. Below is a cheat-sheet on the time and space complexity of typical algorithms.Big O Cheatsheet for Common AlgorithmsSolution to Section 4 Question:By inspecting the functions, we should be able to immediately rank the following polynomials from most complex to least complex with rule 3. Where the square root of n is just n to the power of 0.5.Then by applying rules 2 and 6, we will get the following. Base 3 log can be converted to base 2 with log base conversions. Base 3 log still grows a little bit slower then base 2 logs, and therefore gets ranked after.The rest may look a little bit tricky, but let\u2019s try to unveil their true faces and see where we can put them.First of all, 2 to the power of 2 to the power of n is greater than 2 to the power of n, and the +1 spices it up even more.And then since we know 2 to the power of log(n) with based 2 is equal to n, we can convert the following. The log with 0.001 as exponent grows a little bit more than constants, but less than almost anything else.The one with n to the power of log(log(n)) is actually a variation of the quasi-polynomial, which is greater than polynomial but less than exponential. Since log(n) grows slower than n, the complexity of it is a bit less. The one with the inverse log converges to constant, as 1/log(n) diverges to infinity.The factorials can be represented by multiplications, and thus can be converted to additions outside the logarithmic function. The \u201cn choose 2\u201d can be converted into a polynomial with a cubic term being the largest.And finally, we can rank the functions from the most complex to the least complex.Why BigO doesn\u2019t matter!!! \u2014 WARNING \u2014 !!!Contents discussed here are generally not accepted by most programmers in the world. Discuss it at your own risk in an interview. People actually blogged about how they failed their Google interviews because they questioned the authority, like here.!!! \u2014 WARNING \u2014 !!!Since we have previously learned that the worst case time complexity for quick sort is O(n\u00b2), but O(n * log(n)) for merge sort, merge sort should be faster \u2014 right? Well you probably have guessed that the answer is false. The algorithms are just wired up in a way that makes quick sort the \u201cquick sort\u201d.To demonstrate, check out this trinket.io I made. It compares the time for quick sort and merge sort. I have only managed to test it on arrays with a length up to 10000, but as you can see so far, the time for merge sort grows faster than quick sort. Despite quick sort having a worse case complexity of O(n\u00b2), the likelihood of that is really low. When it comes to the increase in speed quick sort has over merge sort bounded by the O(n * log(n)) complexity, quick sort ends up with a better performance in average.Time Comparison between Quick Sort & Merge SortI have also made the below graph to compare the ratio between the time they take, as it is hard to see them at lower values. And as you can see, the percentage time taken for quick sort is in a descending order.Time Ratio between Quick Sort & Merge SortThe moral of the story is, Big O notation is only a mathematical analysis to provide a reference on the resources consumed by the algorithm. Practically, the results may be different. But it is generally a good practice trying to chop down the complexity of our algorithms, until we run into a case where we know what we are doing.In the end\u2026I like coding, learning new things and sharing them with the community. If there is anything in which you are particularly interested, please let me know. I generally write on web design, software architecture, mathematics and data science. You can find some great articles I have written before if you are interested in any of the topics above.Hope you have a great time learning computer science!!!\n\n                        \n                    \n                    \n                        \n                        \n                    \n\n                    \n                    \n                        \n    \n\n\n\n\n\n    \n    \n\n\n                        \n\n\n        Learn to code for free. freeCodeCamp's open source curriculum has helped more than 40,000 people get jobs as developers. Get started\n    \n\n                    \n                \n                \n                    \n                \n            \n        ", "length": 19706, "excerpt": "Do you really understand Big O? If so, then this will refresh your understanding\nbefore an interview. If not, don\u2019t worry \u2014 come and join us for some endeavors\nin computer science.\n\nIf you have taken some algorithm related courses, you\u2019ve probably heard of the\nterm Big O notation. If you haven\u2019t, we will go over it here, and then get a\ndeeper understanding of what it really is.\n\nBig O notation is one of the most fundamental tools for computer scientists to\nanalyze the cost of an algorithm. It is", "siteName": "freeCodeCamp.org", "publishedTime": "2020-01-16T17:24:00.000Z", "id": "3502474febd3fab6472703984adab2b5e84cbc0b", "url": "https://www.freecodecamp.org/news/big-o-notation-why-it-matters-and-why-it-doesnt-1674cfa8a23c/", "domain": "freecodecamp.org", "date": "2023-12-27T18:07:23.498835", "resultUri": "http://localhost:3000/result/3502474febd3fab6472703984adab2b5e84cbc0b", "query": {"url": ["https://www.freecodecamp.org/news/big-o-notation-why-it-matters-and-why-it-doesnt-1674cfa8a23c/"]}, "meta": {"og": {"site_name": "freeCodeCamp.org", "type": "article", "title": "What is Big O Notation Explained: Space and Time Complexity", "description": "Do you really understand Big O? If so, then this will refresh your understanding before an interview. If not, don\u2019t worry \u2014 come and join us for some endeavors in computer science. If you have taken some algorithm related courses, you\u2019ve probably heard of the term Big O notation. If", "url": "https://www.freecodecamp.org/news/big-o-notation-why-it-matters-and-why-it-doesnt-1674cfa8a23c/", "image": "https://www.freecodecamp.org/news/content/images/2021/06/0_NSxbYAwcC7Qzk7PP.jpg", "image:width": "800", "image:height": "600"}, "twitter": {"card": "summary_large_image", "title": "What is Big O Notation Explained: Space and Time Complexity", "description": "Do you really understand Big O? If so, then this will refresh your understanding before an interview. If not, don\u2019t worry \u2014 come and join us for some endeavors in computer science. If you have taken some algorithm related courses, you\u2019ve probably heard of the term Big O notation. If", "url": "https://www.freecodecamp.org/news/big-o-notation-why-it-matters-and-why-it-doesnt-1674cfa8a23c/", "image": "https://www.freecodecamp.org/news/content/images/2021/06/0_NSxbYAwcC7Qzk7PP.jpg", "label1": "Written by", "data1": "Shen Huang", "label2": "Filed under", "data2": "Big O Notation, Algorithms, Computer Science, Mathematics, Programming", "site": "@freecodecamp"}}}