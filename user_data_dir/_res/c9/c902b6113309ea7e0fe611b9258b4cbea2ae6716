{"title": "Deep-dive into Spark internals and architecture", "byline": "freeCodeCamp.org", "dir": null, "lang": "en", "content": "<div class=\"page\" id=\"readability-page-1\"><div id=\"site-main\">\n<article><h1>Deep-dive into Spark internals and architecture</h1>\n<section>\n<section data-test-label=\"post-content\">\n<p>by Jayvardhan Reddy</p><p><strong><em>Apache Spark</em></strong> is an open-source distributed general-purpose cluster-computing framework. A spark application is a JVM process that\u2019s running a user code using the spark as a 3rd party library.</p><p>As part of this blog, I will be showing the way Spark works on Yarn architecture with an example and the various underlying background processes that are involved such as:</p><ul><li>Spark Context</li><li>Yarn Resource Manager, Application Master &amp; launching of executors (containers).</li><li>Setting up environment variables, job resources.</li><li>CoarseGrainedExecutorBackend &amp; Netty-based RPC.</li><li>SparkListeners.</li><li>Execution of a job (Logical plan, Physical plan).</li><li>Spark-WebUI.</li></ul><h4 id=\"spark-context\"><strong>Spark Context</strong></h4><p>Spark context is the first level of entry point and the heart of any spark application. <strong><em>Spark-shell</em></strong> is nothing but a Scala-based REPL with spark binaries which will create an object sc called spark context.</p><p>We can launch the spark shell as shown below:</p><pre><code>spark-shell --master yarn \\ --conf spark.ui.port=12345 \\ --num-executors 3 \\ --executor-cores 2 \\ --executor-memory 500M</code></pre><p>As part of the spark-shell, we have mentioned the num executors. They indicate the number of worker nodes to be used and the number of cores for each of these worker nodes to execute tasks in parallel.</p><p>Or you can launch spark shell using the default configuration.</p><pre><code>spark-shell --master yarn</code></pre><p>The configurations are present as part of <strong>spark-env.sh</strong></p><figure><img alt=\"-6BLEYtF8novhDmJFdm5jOcRIcU2BnIeIMSY\" height=\"191\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/-6BLEYtF8novhDmJFdm5jOcRIcU2BnIeIMSY\" width=\"751\"/></figure><p>Our Driver program is executed on the Gateway node which is nothing but a spark-shell. It will create a spark context and launch an application.</p><figure><img alt=\"AfCWOl6WV-LTqqOBZhu1vmZgAdxTuEUO0NXm\" height=\"17\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/AfCWOl6WV-LTqqOBZhu1vmZgAdxTuEUO0NXm\" width=\"800\"/></figure><p>The spark context object can be accessed using <strong>sc.</strong></p><figure><img alt=\"XddrUMFGYv0CZtW0KVYll8t6gH0B-vsdJrZJ\" height=\"42\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/XddrUMFGYv0CZtW0KVYll8t6gH0B-vsdJrZJ\" width=\"800\"/></figure><p>After the Spark context is created it waits for the resources. Once the resources are available, Spark context sets up internal services and establishes a connection to a Spark execution environment.</p><h4 id=\"yarn-resource-manager-application-master-launching-of-executors-containers-\"><strong>Yarn Resource Manager, Application Master &amp; launching of executors (containers).</strong></h4><p>Once the Spark context is created it will check with the <strong><em>Cluster Manager</em></strong> and launch the <strong><em>Application Master </em></strong>i.e, launches a container and registers signal handlers<strong><em>.</em></strong></p><figure><img alt=\"cjo3Db6Bu6YAfbynbMqAnUeZwq7b2gyDktzI\" height=\"50\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/cjo3Db6Bu6YAfbynbMqAnUeZwq7b2gyDktzI\" width=\"800\"/></figure><figure><img alt=\"oPN9axnzYJYOD4uVrgnAFHavgelG7PU6qcxC\" height=\"47\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/oPN9axnzYJYOD4uVrgnAFHavgelG7PU6qcxC\" width=\"763\"/></figure><p>Once the Application Master is started it establishes a connection with the Driver.</p><figure><img alt=\"HNXgewZ5xbv1rnAtlAkrVX8cIpM57MEAPUUF\" height=\"51\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/HNXgewZ5xbv1rnAtlAkrVX8cIpM57MEAPUUF\" width=\"749\"/></figure><p>Next, the ApplicationMasterEndPoint triggers a proxy application to connect to the resource manager.</p><figure><img alt=\"qYP4KcyLx47l13m2rK7DmHz1v3HjjvOvfCfc\" height=\"66\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/qYP4KcyLx47l13m2rK7DmHz1v3HjjvOvfCfc\" width=\"800\"/></figure><p>Now, the Yarn Container will perform the below operations as shown in the diagram.</p><figure><img alt=\"Nn-uzm4KF38fk3GEP46x6nHaRY4qEiF0OKZv\" height=\"459\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/Nn-uzm4KF38fk3GEP46x6nHaRY4qEiF0OKZv\" width=\"715\"/><figcaption>Image Credits: <a href=\"https://jaceklaskowski.gitbooks.io/mastering-apache-spark/yarn/spark-yarn-applicationmaster.html\" rel=\"noopener\" target=\"_blank\" title=\"\">jaceklaskowski.gitbooks.io</a></figcaption></figure><p>ii) YarnRMClient will register with the Application Master.</p><figure><img alt=\"L-1dKAks3zKzEG-3LQjUb59y87o32NwCm0CH\" height=\"25\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/L-1dKAks3zKzEG-3LQjUb59y87o32NwCm0CH\" width=\"758\"/></figure><p>iii) YarnAllocator: Will request 3 executor containers, each with 2 cores and 884 MB memory including 384 MB overhead</p><figure><img alt=\"k8JzuqdtEIr30S2i9FDqjLfuCU0fdx1V7hJ9\" height=\"25\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/k8JzuqdtEIr30S2i9FDqjLfuCU0fdx1V7hJ9\" width=\"800\"/></figure><p>iv) AM starts the Reporter Thread</p><figure><img alt=\"0ivLtRFO8U-sXl9auRRFgjaIiCRl7ggf5gEU\" height=\"19\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/0ivLtRFO8U-sXl9auRRFgjaIiCRl7ggf5gEU\" width=\"800\"/></figure><p>Now the Yarn Allocator receives tokens from Driver to launch the Executor nodes and start the containers.</p><figure><img alt=\"ISQkVVySYyDsBWkE5i3OEi-JI602MXyd1SpG\" height=\"114\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/ISQkVVySYyDsBWkE5i3OEi-JI602MXyd1SpG\" width=\"800\"/></figure><h4 id=\"setting-up-environment-variables-job-resources-launching-containers-\"><strong>Setting up environment variables, job resources &amp; launching containers.</strong></h4><p>Every time a container is launched it does the following 3 things in each of these.</p><ul><li>Setting up env variables</li></ul><p>Spark Runtime Environment (SparkEnv) is the runtime environment with Spark\u2019s services that are used to interact with each other in order to establish a distributed computing platform for a Spark application.</p><figure><img alt=\"IuLu5w5LZBGd3LNj5HMD7hUA8M0f9KpdQO4T\" height=\"80\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/IuLu5w5LZBGd3LNj5HMD7hUA8M0f9KpdQO4T\" width=\"747\"/></figure><figure><img alt=\"WvPtwOF6swG4mdHfNdA7SNiaoW1WSAu5b16C\" height=\"205\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/WvPtwOF6swG4mdHfNdA7SNiaoW1WSAu5b16C\" width=\"746\"/></figure><ul><li>Setting up job resources</li></ul><figure><img alt=\"-4Kq6oTpBIzxXxJslrnyja9NvEYOVK0fN8Eo\" height=\"57\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/-4Kq6oTpBIzxXxJslrnyja9NvEYOVK0fN8Eo\" width=\"800\"/></figure><ul><li>Launching container</li></ul><figure><img alt=\"eovWnCKboFLKrabJTkQeheqlSEL1pjvYzyIQ\" height=\"94\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/eovWnCKboFLKrabJTkQeheqlSEL1pjvYzyIQ\" width=\"800\"/></figure><p>YARN executor launch context assigns each executor with an executor id to identify the corresponding executor (via Spark WebUI) and starts a CoarseGrainedExecutorBackend.</p><figure><img alt=\"HwkvxgUxXrbP-sSlod7ww2ON730xBNoCD5OL\" height=\"58\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/HwkvxgUxXrbP-sSlod7ww2ON730xBNoCD5OL\" width=\"800\"/></figure><h4 id=\"coarsegrainedexecutorbackend-netty-based-rpc-\"><strong>CoarseGrainedExecutorBackend &amp; Netty-based RPC.</strong></h4><p>After obtaining resources from Resource Manager, we will see the executor starting up</p><figure><img alt=\"JV2n4sSeorLmRQeqYV5qnX0VipHO7UDpOTDs\" height=\"53\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/JV2n4sSeorLmRQeqYV5qnX0VipHO7UDpOTDs\" width=\"800\"/></figure><p><strong><em>CoarseGrainedExecutorBackend </em></strong>is an ExecutorBackend that controls the lifecycle of a single executor. It sends the executor\u2019s status to the driver.</p><p>When ExecutorRunnable is started, CoarseGrainedExecutorBackend registers the Executor RPC endpoint and signal handlers to communicate with the driver (i.e. with CoarseGrainedScheduler RPC endpoint) and to inform that it is ready to launch tasks.</p><figure><img alt=\"eCIDgTM7qIHET63gN0hT3bURjLreHK0clFvD\" height=\"80\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/eCIDgTM7qIHET63gN0hT3bURjLreHK0clFvD\" width=\"800\"/></figure><p><strong><em>Netty-based RPC - </em></strong>It is used to communicate between worker nodes, spark context, executors.</p><figure><img alt=\"enybOZ6oQRaFwrbnQIbxdcEYf4taktLy8-vO\" height=\"39\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/enybOZ6oQRaFwrbnQIbxdcEYf4taktLy8-vO\" width=\"800\"/></figure><p>NettyRPCEndPoint is used to track the result status of the worker node.</p><p>RpcEndpointAddress is the logical address for an endpoint registered to an RPC Environment, with RpcAddress and name.</p><p>It is in the format as shown below:</p><figure><img alt=\"kLkRsIqy4YhYzRXyUuDroQHeNgxu7Vozk1Gp\" height=\"23\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/kLkRsIqy4YhYzRXyUuDroQHeNgxu7Vozk1Gp\" width=\"744\"/></figure><p>This is the first moment when CoarseGrainedExecutorBackend initiates communication with the driver available at driverUrl through RpcEnv.</p><figure><img alt=\"de85dQifuBDHqUZdqmXrl14e3LYMJuHvQZKP\" height=\"17\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/de85dQifuBDHqUZdqmXrl14e3LYMJuHvQZKP\" width=\"800\"/></figure><h4 id=\"sparklisteners\"><strong>SparkListeners</strong></h4><figure><img alt=\"AIHRmLNMZpTUpgYFqylC0FeveeYLmLjbnZbo\" height=\"323\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/AIHRmLNMZpTUpgYFqylC0FeveeYLmLjbnZbo\" width=\"800\"/><figcaption>Image Credits:<a href=\"https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-scheduler-LiveListenerBus.html\" rel=\"noopener\" target=\"_blank\" title=\"\"> jaceklaskowski.gitbooks.io</a></figcaption></figure><p>SparkListener (Scheduler listener) is a class that listens to execution events from Spark\u2019s DAGScheduler and logs all the event information of an application such as the executor, driver allocation details along with jobs, stages, and tasks and other environment properties changes.</p><p>SparkContext starts the LiveListenerBus that resides inside the driver. It registers JobProgressListener with LiveListenerBus which collects all the data to show the statistics in spark UI.</p><p>By default, only the listener for WebUI would be enabled but if we want to add any other listeners then we can use <strong>spark.extraListeners.</strong></p><p>Spark comes with two listeners that showcase most of the activities</p><p>i) StatsReportListener</p><p>ii) EventLoggingListener</p><p><strong><em>EventLoggingListener:</em></strong><em> </em>If you want to analyze further the performance of your applications beyond what is available as part of the Spark history server then you can process the event log data. Spark Event Log records info on processed jobs/stages/tasks. It can be enabled as shown below...</p><figure><img alt=\"haTAg5LO0bzKsjjXZbpE26AkR2Is2nLBbOzF\" height=\"128\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/haTAg5LO0bzKsjjXZbpE26AkR2Is2nLBbOzF\" width=\"750\"/></figure><p>The event log file can be read as shown below</p><ul><li>The Spark driver logs into job workload/perf metrics in the spark.evenLog.dir directory as JSON files.</li><li>There is one file per application, the file names contain the application id (therefore including a timestamp) application_1540458187951_38909.</li></ul><figure><img alt=\"ecY6tTy-i4s3mmYZoAhraM2KENWWtVgJD8wY\" height=\"94\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/ecY6tTy-i4s3mmYZoAhraM2KENWWtVgJD8wY\" width=\"800\"/></figure><p>It shows the type of events and the number of entries for each.</p><figure><img alt=\"ANujSUpy0IQexpkoae-HfMFckmOPwCLV1-ve\" height=\"420\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/ANujSUpy0IQexpkoae-HfMFckmOPwCLV1-ve\" width=\"749\"/></figure><p>Now, let\u2019s add <strong><em>StatsReportListener </em></strong>to the spark.extraListeners<strong> </strong>and check the status of the job.</p><p>Enable INFO logging level for org.apache.spark.scheduler.StatsReportListener logger to see Spark events.</p><figure><img alt=\"B7HGuBTSYxAa6Ep8B47LncJicPMXZiT6bkPP\" height=\"52\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/B7HGuBTSYxAa6Ep8B47LncJicPMXZiT6bkPP\" width=\"750\"/></figure><p>To enable the listener, you register it to SparkContext. It can be done in two ways.</p><p>i) Using SparkContext.addSparkListener(listener: SparkListener) method inside your Spark application.</p><p>Click on the link to implement custom listeners - <a href=\"https://stackoverflow.com/questions/24463055/how-to-implement-custom-job-listener-tracker-in-spark\" rel=\"noopener\"><strong>CustomListener</strong></a></p><p>ii) Using the conf command-line option</p><figure><img alt=\"eOkpNJ380lOHphroPJjCYtKcBTrnwCMncM7G\" height=\"46\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/eOkpNJ380lOHphroPJjCYtKcBTrnwCMncM7G\" width=\"800\"/></figure><p>Let\u2019s read a sample file and perform a count operation to see the StatsReportListener.</p><figure><img alt=\"C5Aro1tHenM1CulhoUqtcvbg487-4TfhrNfR\" height=\"125\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/C5Aro1tHenM1CulhoUqtcvbg487-4TfhrNfR\" width=\"800\"/></figure><h4 id=\"execution-of-a-job-logical-plan-physical-plan-\"><strong>Execution of a job (Logical plan, Physical plan).</strong></h4><p>In Spark, RDD (<em>resilient distributed dataset</em>) is the first level of the abstraction layer. It is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDDs can be created in 2 ways.</p><p><strong>i) P<em>arallelizing</em> an existing collection in your driver program</strong></p><figure><img alt=\"URNJDr-DZdPfXLVbiWQrEe2-PEX0-p67g1mw\" height=\"50\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/URNJDr-DZdPfXLVbiWQrEe2-PEX0-p67g1mw\" width=\"748\"/></figure><p><strong>ii) Referencing a dataset in an external storage system</strong></p><figure><img alt=\"mFYIkS67sITmSz7SV1MOfFDaMVh-jWQB4ARv\" height=\"38\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/mFYIkS67sITmSz7SV1MOfFDaMVh-jWQB4ARv\" width=\"800\"/></figure><p>RDDs are created either by using a file in the Hadoop file system, or an existing Scala collection in the driver program, and transforming it.</p><p>Let\u2019s take a sample snippet as shown below</p><figure><img alt=\"H4qJqN74iIRbDBWx6qhi9Uuqr7EFNTKYT2Df\" height=\"74\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/H4qJqN74iIRbDBWx6qhi9Uuqr7EFNTKYT2Df\" width=\"769\"/></figure><p>The execution of the above snippet takes place in 2 phases.</p><p><strong><em>6.1 Logical Plan:</em> </strong>In this phase, an RDD is created using a set of transformations, It keeps track of those transformations in the driver program by building a computing chain (a series of RDD)as a Graph of transformations to produce one RDD called a <strong><em>Lineage Graph</em></strong>.</p><p>Transformations can further be divided into 2 types</p><ul><li><strong>Narrow transformation: </strong>A pipeline of operations that can be executed as one stage and does not require the data to be shuffled across the partitions \u2014 for example, Map, filter, etc..</li></ul><figure><img alt=\"y0iv0YErJYvRwvm8BEEqw9KhIVM5IM54FLJZ\" height=\"25\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/y0iv0YErJYvRwvm8BEEqw9KhIVM5IM54FLJZ\" width=\"800\"/></figure><p>Now the data will be read into the driver using the broadcast variable.</p><figure><img alt=\"LpuhSTa3XXdggW9J4e52QXTSNGKIbBzwOYdY\" height=\"81\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/LpuhSTa3XXdggW9J4e52QXTSNGKIbBzwOYdY\" width=\"800\"/></figure><ul><li><strong>Wide transformation:</strong> Here each operation requires the data to be shuffled, henceforth for each wide transformation a new stage will be created \u2014 for example, reduceByKey, etc..</li></ul><figure><img alt=\"8Wzx99ex1SXX0WUPWTVPjzUTLbjS1j0QawFG\" height=\"65\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/8Wzx99ex1SXX0WUPWTVPjzUTLbjS1j0QawFG\" width=\"800\"/></figure><p>We can view the lineage graph by using <strong><em>toDebugString</em></strong></p><figure><img alt=\"Tiz7zB4BUl8KhLcyE9LFZMKzwt3RWt1Symj9\" height=\"174\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/Tiz7zB4BUl8KhLcyE9LFZMKzwt3RWt1Symj9\" width=\"800\"/></figure><p><strong><em>6.2 Physical Plan:</em></strong><em> </em>In this phase, once we trigger an action on the RDD, The <strong><em>DAG Scheduler</em></strong> looks at RDD lineage and comes up with the best execution plan with stages and tasks together with TaskSchedulerImpl and execute the job into a set of tasks parallelly.</p><figure><img alt=\"YYUBKfaYaYQiE7zrsdUOCNwSkcs8fll7FhyF\" height=\"21\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/YYUBKfaYaYQiE7zrsdUOCNwSkcs8fll7FhyF\" width=\"765\"/></figure><p>Once we perform an action operation, the SparkContext triggers a job and registers the RDD until the first stage (i.e, before any wide transformations) as part of the DAGScheduler.</p><figure><img alt=\"5zSZlLhIownDYA1xRAuMjyG4waHL4M654lNn\" height=\"128\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/5zSZlLhIownDYA1xRAuMjyG4waHL4M654lNn\" width=\"750\"/></figure><p>Now before moving onto the next stage (Wide transformations), it will check if there are any partition data that is to be shuffled and if it has any missing parent operation results on which it depends, if any such stage is missing then it re-executes that part of the operation by making use of the DAG( Directed Acyclic Graph) which makes it Fault tolerant.</p><figure><img alt=\"RbSQEIy9nbRolwQPc0lygIxhCa9odYAgaaP0\" height=\"87\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/RbSQEIy9nbRolwQPc0lygIxhCa9odYAgaaP0\" width=\"800\"/></figure><p>In the case of missing tasks, it assigns tasks to executors.</p><figure><img alt=\"q2UCiTrCdl1D4Jr7uM47IbdStmXvOSW2LrNc\" height=\"77\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/q2UCiTrCdl1D4Jr7uM47IbdStmXvOSW2LrNc\" width=\"800\"/></figure><p>Each task is assigned to CoarseGrainedExecutorBackend of the executor.</p><figure><img alt=\"QcoWEHGMvmiF5lBbmAN4oT1y5cOUt6kEZ0OB\" height=\"99\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/QcoWEHGMvmiF5lBbmAN4oT1y5cOUt6kEZ0OB\" width=\"728\"/></figure><p>It gets the block info from the Namenode.</p><figure><img alt=\"smtnxaTQw1B5rzIQ0aoDgfcpZh4PL5GPhcrH\" height=\"43\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/smtnxaTQw1B5rzIQ0aoDgfcpZh4PL5GPhcrH\" width=\"800\"/></figure><p>now, it performs the computation and returns the result.</p><figure><img alt=\"pSG-NIrRKOV-LxRxXGLgbi2pizwgx2wiCZtB\" height=\"107\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/pSG-NIrRKOV-LxRxXGLgbi2pizwgx2wiCZtB\" width=\"800\"/></figure><p>Next, the DAGScheduler looks for the newly runnable stages and triggers the next stage (reduceByKey) operation.</p><figure><img alt=\"o5SPhS1d8o1fG5klmiupU--BEc52ZvmaWbbA\" height=\"37\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/o5SPhS1d8o1fG5klmiupU--BEc52ZvmaWbbA\" width=\"800\"/></figure><p>The ShuffleBlockFetcherIterator gets the blocks to be shuffled.</p><figure><img alt=\"aUU1-Z-0bbW8vZRS43DhJrwffkR6V0pHos2z\" height=\"97\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/aUU1-Z-0bbW8vZRS43DhJrwffkR6V0pHos2z\" width=\"720\"/></figure><p>Now the reduce operation is divided into 2 tasks and executed.</p><figure><img alt=\"U4WfZPLsoa76XL9bTz2xadadQD5cpZAVrTpq\" height=\"125\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/U4WfZPLsoa76XL9bTz2xadadQD5cpZAVrTpq\" width=\"800\"/></figure><p>On completion of each task, the executor returns the result back to the driver.</p><figure><img alt=\"-8Pwz7cMv3GLJwRJSCZ42Bm-NeD98E91Zf1r\" height=\"49\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/-8Pwz7cMv3GLJwRJSCZ42Bm-NeD98E91Zf1r\" width=\"800\"/></figure><p>Once the Job is finished the result is displayed.</p><figure><img alt=\"m3KpbqF4utduxP0wHb634aOREZf2LXehYoyu\" height=\"48\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/m3KpbqF4utduxP0wHb634aOREZf2LXehYoyu\" width=\"745\"/></figure><h4 id=\"spark-webui\"><strong>Spark-WebUI</strong></h4><p>Spark-UI helps in understanding the code execution flow and the time taken to complete a particular job. The visualization helps in finding out any underlying problems that take place during the execution and optimizing the spark application further.</p><p>We will see the Spark-UI visualization as part of the previous <strong>step 6.</strong></p><p>Once the job is completed you can see the job details such as the number of stages, the number of tasks that were scheduled during the job execution of a Job.</p><figure><img alt=\"oMGL38wBVkMpbyBwDz8oUjn4J8HgUa6FkQcy\" height=\"255\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/oMGL38wBVkMpbyBwDz8oUjn4J8HgUa6FkQcy\" width=\"800\"/></figure><p>On clicking the completed jobs we can view the DAG visualization i.e, the different wide and narrow transformations as part of it.</p><figure><img alt=\"QbHRUFsfBCmjW5Wjx-iwl1RKFG181TPvsso2\" height=\"316\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/QbHRUFsfBCmjW5Wjx-iwl1RKFG181TPvsso2\" width=\"800\"/></figure><p>You can see the execution time taken by each stage.</p><figure><img alt=\"EnqEsKsm7oOpyCmYjJUVJ78V6dP0tZwDYfr6\" height=\"197\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/EnqEsKsm7oOpyCmYjJUVJ78V6dP0tZwDYfr6\" width=\"800\"/></figure><p>On clicking on a Particular stage as part of the job, it will show the complete details as to where the data blocks are residing, data size, the executor used, memory utilized and the time taken to complete a particular task. It also shows the number of shuffles that take place.</p><figure><img alt=\"XABdREC97TPKB3l1tDHuvBT9fUo9oGnjFLm4\" height=\"725\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/XABdREC97TPKB3l1tDHuvBT9fUo9oGnjFLm4\" width=\"800\"/></figure><p>Further, we can click on the Executors tab to view the Executor and driver used.</p><figure><img alt=\"VAWPvAI4Jst5MN4Z60ed29qgrhAgs2fy5Yd0\" height=\"285\" loading=\"lazy\" src=\"https://cdn-media-1.freecodecamp.org/images/VAWPvAI4Jst5MN4Z60ed29qgrhAgs2fy5Yd0\" width=\"800\"/></figure><p>Now that we have seen how Spark works internally, you can determine the flow of execution by making use of Spark UI, logs and tweaking the Spark EventListeners to determine optimal solution on the submission of a Spark job.</p><p><strong>Note<em>:</em></strong> The commands that were executed related to this post are added as part of my <a href=\"https://github.com/Jayvardhan-Reddy/BigData-Ecosystem-Architecture\" rel=\"noopener\">GIT </a>account.</p><p>Similarly, you can also read more here:</p><ul><li><a href=\"https://medium.freecodecamp.org/an-in-depth-introduction-to-sqoop-architecture-ad4ae0532583\" rel=\"noopener\">Sqoop Architecture in Depth</a> with <strong>code.</strong></li><li><a href=\"https://medium.com/plumbersofdatascience/hdfs-architecture-in-depth-1edb822b95fa\" rel=\"noopener\">HDFS Architecture in Depth</a> with <strong>code</strong>.</li><li><a href=\"https://medium.com/plumbersofdatascience/hive-architecture-in-depth-ba44e8946cbc\" rel=\"noopener\">Hive Architecture in Depth</a> with <strong>code</strong>.</li></ul><p>If you would like too, you can connect with me on LinkedIn \u2014 <a href=\"https://www.linkedin.com/in/jayvardhan-reddy-vanchireddy\" rel=\"noopener\">Jayvardhan Reddy</a>.</p><p>If you enjoyed reading it, you can click the clap and let others know about it. If you would like me to add anything else, please feel free to leave a response ?</p>\n</section>\n<hr/>\n<p>\n        Learn to code for free. freeCodeCamp's open source curriculum has helped more than 40,000 people get jobs as developers. <a href=\"https://www.freecodecamp.org/learn/\" id=\"learn-to-code-cta\" rel=\"noopener noreferrer\" target=\"_blank\">Get started</a>\n</p>\n</section>\n</article>\n</div></div>", "textContent": "\n            \n                \n                \n                \n                    \n                \n                \n                    \n                            \nby Jayvardhan ReddyApache Spark is an open-source distributed general-purpose cluster-computing framework. A spark application is a JVM process that\u2019s running a user code using the spark as a 3rd party library.As part of this blog, I will be showing the way Spark works on Yarn architecture with an example and the various underlying background processes that are involved such as:Spark ContextYarn Resource Manager, Application Master & launching of executors (containers).Setting up environment variables, job resources.CoarseGrainedExecutorBackend & Netty-based RPC.SparkListeners.Execution of a job (Logical plan, Physical plan).Spark-WebUI.Spark ContextSpark context is the first level of entry point and the heart of any spark application. Spark-shell is nothing but a Scala-based REPL with spark binaries which will create an object sc called spark context.We can launch the spark shell as shown below:spark-shell --master yarn \\ --conf spark.ui.port=12345 \\ --num-executors 3 \\ --executor-cores 2 \\ --executor-memory 500MAs part of the spark-shell, we have mentioned the num executors. They indicate the number of worker nodes to be used and the number of cores for each of these worker nodes to execute tasks in parallel.Or you can launch spark shell using the default configuration.spark-shell --master yarnThe configurations are present as part of spark-env.shOur Driver program is executed on the Gateway node which is nothing but a spark-shell. It will create a spark context and launch an application.The spark context object can be accessed using sc.After the Spark context is created it waits for the resources. Once the resources are available, Spark context sets up internal services and establishes a connection to a Spark execution environment.Yarn Resource Manager, Application Master & launching of executors (containers).Once the Spark context is created it will check with the Cluster Manager and launch the Application Master i.e, launches a container and registers signal handlers.Once the Application Master is started it establishes a connection with the Driver.Next, the ApplicationMasterEndPoint triggers a proxy application to connect to the resource manager.Now, the Yarn Container will perform the below operations as shown in the diagram.Image Credits: jaceklaskowski.gitbooks.ioii) YarnRMClient will register with the Application Master.iii) YarnAllocator: Will request 3 executor containers, each with 2 cores and 884 MB memory including 384 MB overheadiv) AM starts the Reporter ThreadNow the Yarn Allocator receives tokens from Driver to launch the Executor nodes and start the containers.Setting up environment variables, job resources & launching containers.Every time a container is launched it does the following 3 things in each of these.Setting up env variablesSpark Runtime Environment (SparkEnv) is the runtime environment with Spark\u2019s services that are used to interact with each other in order to establish a distributed computing platform for a Spark application.Setting up job resourcesLaunching containerYARN executor launch context assigns each executor with an executor id to identify the corresponding executor (via Spark WebUI) and starts a CoarseGrainedExecutorBackend.CoarseGrainedExecutorBackend & Netty-based RPC.After obtaining resources from Resource Manager, we will see the executor starting upCoarseGrainedExecutorBackend is an ExecutorBackend that controls the lifecycle of a single executor. It sends the executor\u2019s status to the driver.When ExecutorRunnable is started, CoarseGrainedExecutorBackend registers the Executor RPC endpoint and signal handlers to communicate with the driver (i.e. with CoarseGrainedScheduler RPC endpoint) and to inform that it is ready to launch tasks.Netty-based RPC - It is used to communicate between worker nodes, spark context, executors.NettyRPCEndPoint is used to track the result status of the worker node.RpcEndpointAddress is the logical address for an endpoint registered to an RPC Environment, with RpcAddress and name.It is in the format as shown below:This is the first moment when CoarseGrainedExecutorBackend initiates communication with the driver available at driverUrl through RpcEnv.SparkListenersImage Credits: jaceklaskowski.gitbooks.ioSparkListener (Scheduler listener) is a class that listens to execution events from Spark\u2019s DAGScheduler and logs all the event information of an application such as the executor, driver allocation details along with jobs, stages, and tasks and other environment properties changes.SparkContext starts the LiveListenerBus that resides inside the driver. It registers JobProgressListener with LiveListenerBus which collects all the data to show the statistics in spark UI.By default, only the listener for WebUI would be enabled but if we want to add any other listeners then we can use spark.extraListeners.Spark comes with two listeners that showcase most of the activitiesi) StatsReportListenerii) EventLoggingListenerEventLoggingListener: If you want to analyze further the performance of your applications beyond what is available as part of the Spark history server then you can process the event log data. Spark Event Log records info on processed jobs/stages/tasks. It can be enabled as shown below...The event log file can be read as shown belowThe Spark driver logs into job workload/perf metrics in the spark.evenLog.dir directory as JSON files.There is one file per application, the file names contain the application id (therefore including a timestamp) application_1540458187951_38909.It shows the type of events and the number of entries for each.Now, let\u2019s add StatsReportListener to the spark.extraListeners and check the status of the job.Enable INFO logging level for org.apache.spark.scheduler.StatsReportListener logger to see Spark events.To enable the listener, you register it to SparkContext. It can be done in two ways.i) Using SparkContext.addSparkListener(listener: SparkListener) method inside your Spark application.Click on the link to implement custom listeners - CustomListenerii) Using the conf command-line optionLet\u2019s read a sample file and perform a count operation to see the StatsReportListener.Execution of a job (Logical plan, Physical plan).In Spark, RDD (resilient distributed dataset) is the first level of the abstraction layer. It is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDDs can be created in 2 ways.i) Parallelizing an existing collection in your driver programii) Referencing a dataset in an external storage systemRDDs are created either by using a file in the Hadoop file system, or an existing Scala collection in the driver program, and transforming it.Let\u2019s take a sample snippet as shown belowThe execution of the above snippet takes place in 2 phases.6.1 Logical Plan: In this phase, an RDD is created using a set of transformations, It keeps track of those transformations in the driver program by building a computing chain (a series of RDD)as a Graph of transformations to produce one RDD called a Lineage Graph.Transformations can further be divided into 2 typesNarrow transformation: A pipeline of operations that can be executed as one stage and does not require the data to be shuffled across the partitions \u2014 for example, Map, filter, etc..Now the data will be read into the driver using the broadcast variable.Wide transformation: Here each operation requires the data to be shuffled, henceforth for each wide transformation a new stage will be created \u2014 for example, reduceByKey, etc..We can view the lineage graph by using toDebugString6.2 Physical Plan: In this phase, once we trigger an action on the RDD, The DAG Scheduler looks at RDD lineage and comes up with the best execution plan with stages and tasks together with TaskSchedulerImpl and execute the job into a set of tasks parallelly.Once we perform an action operation, the SparkContext triggers a job and registers the RDD until the first stage (i.e, before any wide transformations) as part of the DAGScheduler.Now before moving onto the next stage (Wide transformations), it will check if there are any partition data that is to be shuffled and if it has any missing parent operation results on which it depends, if any such stage is missing then it re-executes that part of the operation by making use of the DAG( Directed Acyclic Graph) which makes it Fault tolerant.In the case of missing tasks, it assigns tasks to executors.Each task is assigned to CoarseGrainedExecutorBackend of the executor.It gets the block info from the Namenode.now, it performs the computation and returns the result.Next, the DAGScheduler looks for the newly runnable stages and triggers the next stage (reduceByKey) operation.The ShuffleBlockFetcherIterator gets the blocks to be shuffled.Now the reduce operation is divided into 2 tasks and executed.On completion of each task, the executor returns the result back to the driver.Once the Job is finished the result is displayed.Spark-WebUISpark-UI helps in understanding the code execution flow and the time taken to complete a particular job. The visualization helps in finding out any underlying problems that take place during the execution and optimizing the spark application further.We will see the Spark-UI visualization as part of the previous step 6.Once the job is completed you can see the job details such as the number of stages, the number of tasks that were scheduled during the job execution of a Job.On clicking the completed jobs we can view the DAG visualization i.e, the different wide and narrow transformations as part of it.You can see the execution time taken by each stage.On clicking on a Particular stage as part of the job, it will show the complete details as to where the data blocks are residing, data size, the executor used, memory utilized and the time taken to complete a particular task. It also shows the number of shuffles that take place.Further, we can click on the Executors tab to view the Executor and driver used.Now that we have seen how Spark works internally, you can determine the flow of execution by making use of Spark UI, logs and tweaking the Spark EventListeners to determine optimal solution on the submission of a Spark job.Note: The commands that were executed related to this post are added as part of my GIT account.Similarly, you can also read more here:Sqoop Architecture in Depth with code.HDFS Architecture in Depth with code.Hive Architecture in Depth with code.If you would like too, you can connect with me on LinkedIn \u2014 Jayvardhan Reddy.If you enjoyed reading it, you can click the clap and let others know about it. If you would like me to add anything else, please feel free to leave a response ?\n\n                        \n                    \n                    \n\n                    \n                    \n                        \n    \n\n\n\n\n\n    \n    \n\n\n                        \n\n\n        Learn to code for free. freeCodeCamp's open source curriculum has helped more than 40,000 people get jobs as developers. Get started\n    \n\n                    \n                \n                \n                    \n                \n            \n        ", "length": 11367, "excerpt": "by Jayvardhan Reddy\n\nDeep-dive into Spark internals and architecture\nImage Credits: spark.apache.org\n[https://spark.apache.org/docs/latest/cluster-overview.html]Apache Spark is an open-source distributed general-purpose cluster-computing\nframework. A spark application is a JVM process that\u2019s running a user code using\nthe spark as a 3rd party library.\n\nAs part of this blog, I will be showing the way Spark works on Yarn architecture\nwith an example and the various underlying background processes t", "siteName": "freeCodeCamp.org", "publishedTime": "2019-05-14T20:18:53.000Z", "id": "c902b6113309ea7e0fe611b9258b4cbea2ae6716", "url": "https://www.freecodecamp.org/news/deep-dive-into-spark-internals-and-architecture-f6e32045393b/", "domain": "freecodecamp.org", "date": "2023-12-27T18:09:55.127144", "resultUri": "http://localhost:3000/result/c902b6113309ea7e0fe611b9258b4cbea2ae6716", "query": {"url": ["https://www.freecodecamp.org/news/deep-dive-into-spark-internals-and-architecture-f6e32045393b/"]}, "meta": {"og": {"site_name": "freeCodeCamp.org", "type": "article", "title": "Deep-dive into Spark internals and architecture", "description": "by Jayvardhan Reddy Deep-dive into Spark internals and architecture Image Credits: spark.apache.org [https://spark.apache.org/docs/latest/cluster-overview.html]Apache Spark is an open-source distributed general-purpose cluster-computing framework. A spark application is a JVM process that\u2019s running a user code using the spark as a 3rd party library. As part of this blog, I will be showing", "url": "https://www.freecodecamp.org/news/deep-dive-into-spark-internals-and-architecture-f6e32045393b/", "image": "https://cdn-media-1.freecodecamp.org/images/1*EzZs4uEuO30lV51KV07_RA.png", "image:width": "596", "image:height": "286"}, "twitter": {"card": "summary_large_image", "title": "Deep-dive into Spark internals and architecture", "description": "by Jayvardhan Reddy Deep-dive into Spark internals and architecture Image Credits: spark.apache.org [https://spark.apache.org/docs/latest/cluster-overview.html]Apache Spark is an open-source distributed general-purpose cluster-computing framework. A spark application is a JVM process that\u2019s running a user code using the spark as a 3rd party library. As part of this blog, I will be showing", "url": "https://www.freecodecamp.org/news/deep-dive-into-spark-internals-and-architecture-f6e32045393b/", "image": "https://cdn-media-1.freecodecamp.org/images/1*EzZs4uEuO30lV51KV07_RA.png", "label1": "Written by", "data1": "freeCodeCamp.org", "label2": "Filed under", "data2": "Spark, Data Science, Technology, Artificial Intelligence, Programming", "site": "@freecodecamp"}}}