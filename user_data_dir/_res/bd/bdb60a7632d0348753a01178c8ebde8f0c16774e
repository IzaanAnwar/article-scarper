{"title": "How I get options data for free", "byline": "freeCodeCamp.org", "dir": null, "lang": "en", "content": "<div class=\"page\" id=\"readability-page-1\"><div id=\"site-main\">\n<article><h1>How I get options data for free</h1>\n<section>\n<section data-test-label=\"post-content\">\n<p>by Harry Sauers</p><h4 id=\"an-introduction-to-web-scraping-for-finance\">An introduction to web scraping for finance</h4><p>Ever wished you could access historical options data, but got blocked by a paywall? What if you just want it for research, fun, or to develop a personal trading strategy?</p><p>In this tutorial, you\u2019ll learn how to use Python and BeautifulSoup to scrape financial data from the Web and build your own dataset.</p><h3 id=\"getting-started\"><strong>Getting Started</strong></h3><p>You should have at least a working knowledge of Python and Web technologies before beginning this tutorial. To build these up, I highly recommend checking out a site like <a href=\"https://www.codecademy.com/\" rel=\"noopener\">codecademy</a> to learn new skills or brush up on old ones.</p><p>First, let\u2019s spin up your favorite IDE. Normally, I use <a href=\"https://www.jetbrains.com/help/pycharm/installation-guide.html\" rel=\"noopener\">PyCharm</a> but, for a quick script like this <a href=\"https://repl.it/\" rel=\"noopener\">Repl.it</a> will do the job too. Add a quick print (\u201cHello world\u201d) to ensure your environment is set up correctly.</p><p>Now we need to figure out a data source.</p><p>Unfortunately, <a href=\"http://www.cboe.com/delayedquote/quote-table\" rel=\"noopener\">Cboe\u2019s awesome options chain data</a> is pretty locked down, even for current delayed quotes. Luckily, Yahoo Finance has solid enough options data <a href=\"https://finance.yahoo.com/quote/SPY/options?p=SPY\" rel=\"noopener\">here</a>. We\u2019ll use it for this tutorial, as web scrapers often need some content awareness, but it is easily adaptable for any data source you want.</p><h3 id=\"dependencies\"><strong>Dependencies</strong></h3><p>We don\u2019t need many external dependencies. We just need the Requests and BeautifulSoup modules in Python. Add these at the top of your program:</p><pre><code>from bs4 import BeautifulSoupimport requests</code></pre><p>Create a <code>main</code> method:</p><pre><code>def main():  print(\u201cHello World!\u201d)if __name__ == \u201c__main__\u201d:  main()</code></pre><h3 id=\"scraping-html\">Scraping HTML</h3><p>Now you\u2019re ready to start scraping! Inside <code>main()</code>, add these lines to fetch the page\u2019s full <code>HTML</code>:</p><pre><code>data_url = \u201chttps://finance.yahoo.com/quote/SPY/options\"data_html = requests.get(data_url).contentprint(data_html)</code></pre><p>This fetches the page\u2019s full <code>HTML</code> content, so we can find the data we want in it. Feel free to give it a run and observe the output.</p><p>Feel free to comment out print statements as you go \u2014 these are just there to help you understand what the program is doing at any given step.</p><p>BeautifulSoup is the perfect tool for working with <code>HTML</code> data in Python. Let\u2019s narrow down the <code>HTML</code> to just the options pricing tables so we can better understand it:</p><pre><code> content = BeautifulSoup(data_html, \u201chtml.parser\u201d) # print(content)</code></pre><pre><code> options_tables = content.find_all(\u201ctable\u201d) print(options_tables)</code></pre><p>That\u2019s still quite a bit of <code>HTML</code> \u2014 we can\u2019t get much out of that, and Yahoo\u2019s code isn\u2019t the most friendly to web scrapers. Let\u2019s break it down into two tables, for calls and puts:</p><pre><code> options_tables = [] tables = content.find_all(\u201ctable\u201d) for i in range(0, len(content.find_all(\u201ctable\u201d))):   options_tables.append(tables[i])</code></pre><pre><code> print(options_tables)</code></pre><p>Yahoo\u2019s data contains options that are pretty deep in- and out-of-the-money, which might be great for certain purposes. I\u2019m only interested in near-the-money options, namely the two calls and two puts closest to the current price.</p><p>Let\u2019s find these, using BeautifulSoup and Yahoo\u2019s differential table entries for in-the-money and out-of-the-money options:</p><pre><code>expiration = datetime.datetime.fromtimestamp(int(datestamp)).strftime(\u201c%Y-%m-%d\u201d)</code></pre><pre><code>calls = options_tables[0].find_all(\u201ctr\u201d)[1:] # first row is header</code></pre><pre><code>itm_calls = []otm_calls = []</code></pre><pre><code>for call_option in calls:    if \u201cin-the-money\u201d in str(call_option):  itm_calls.append(call_option)  else:    otm_calls.append(call_option)</code></pre><pre><code>itm_call = itm_calls[-1]otm_call = otm_calls[0]</code></pre><pre><code>print(str(itm_call) + \u201c \\n\\n \u201c + str(otm_call))</code></pre><p>Now, we have the table entries for the two options nearest to the money in <code>HTML</code>. Let\u2019s scrape the pricing data, volume, and implied volatility from the first call option:</p><pre><code> itm_call_data = [] for td in BeautifulSoup(str(itm_call), \u201chtml.parser\u201d).find_all(\u201ctd\u201d):   itm_call_data.append(td.text)</code></pre><pre><code>print(itm_call_data)</code></pre><pre><code>itm_call_info = {\u2018contract\u2019: itm_call_data[0], \u2018strike\u2019: itm_call_data[2], \u2018last\u2019: itm_call_data[3],  \u2018bid\u2019: itm_call_data[4], \u2018ask\u2019: itm_call_data[5], \u2018volume\u2019: itm_call_data[8], \u2018iv\u2019: itm_call_data[10]}</code></pre><pre><code>print(itm_call_info)</code></pre><p>Adapt this code for the next call option:</p><pre><code># otm callotm_call_data = []for td in BeautifulSoup(str(otm_call), \u201chtml.parser\u201d).find_all(\u201ctd\u201d):  otm_call_data.append(td.text)</code></pre><pre><code># print(otm_call_data)</code></pre><pre><code>otm_call_info = {\u2018contract\u2019: otm_call_data[0], \u2018strike\u2019: otm_call_data[2], \u2018last\u2019: otm_call_data[3],  \u2018bid\u2019: otm_call_data[4], \u2018ask\u2019: otm_call_data[5], \u2018volume\u2019: otm_call_data[8], \u2018iv\u2019: otm_call_data[10]}</code></pre><pre><code>print(otm_call_info)</code></pre><p>Give your program a run!</p><p>You now have dictionaries of the two near-the-money call options. It\u2019s enough just to scrape the table of put options for this same data:</p><pre><code>puts = options_tables[1].find_all(\"tr\")[1:]  # first row is header</code></pre><pre><code>itm_puts = []  otm_puts = []</code></pre><pre><code>for put_option in puts:    if \"in-the-money\" in str(put_option):      itm_puts.append(put_option)    else:      otm_puts.append(put_option)</code></pre><pre><code>itm_put = itm_puts[0]  otm_put = otm_puts[-1]</code></pre><pre><code># print(str(itm_put) + \" \\n\\n \" + str(otm_put) + \"\\n\\n\")</code></pre><pre><code>itm_put_data = []  for td in BeautifulSoup(str(itm_put), \"html.parser\").find_all(\"td\"):    itm_put_data.append(td.text)</code></pre><pre><code># print(itm_put_data)</code></pre><pre><code>itm_put_info = {'contract': itm_put_data[0],                  'last_trade': itm_put_data[1][:10],                  'strike': itm_put_data[2], 'last': itm_put_data[3],                   'bid': itm_put_data[4], 'ask': itm_put_data[5], 'volume': itm_put_data[8], 'iv': itm_put_data[10]}</code></pre><pre><code># print(itm_put_info)</code></pre><pre><code># otm put  otm_put_data = []  for td in BeautifulSoup(str(otm_put), \"html.parser\").find_all(\"td\"):    otm_put_data.append(td.text)</code></pre><pre><code># print(otm_put_data)</code></pre><pre><code>otm_put_info = {'contract': otm_put_data[0],                  'last_trade': otm_put_data[1][:10],                  'strike': otm_put_data[2], 'last': otm_put_data[3],                   'bid': otm_put_data[4], 'ask': otm_put_data[5], 'volume': otm_put_data[8], 'iv': otm_put_data[10]}</code></pre><p>Congratulations! You just scraped data for all near-the-money options of the S&amp;P 500 ETF, and can view them like this:</p><pre><code> print(\"\\n\\n\") print(itm_call_info) print(otm_call_info) print(itm_put_info) print(otm_put_info)</code></pre><p>Give your program a run \u2014 you should get data like this printed to the console:</p><pre><code>{\u2018contract\u2019: \u2018SPY190417C00289000\u2019, \u2018last_trade\u2019: \u20182019\u201304\u201315\u2019, \u2018strike\u2019: \u2018289.00\u2019, \u2018last\u2019: \u20181.46\u2019, \u2018bid\u2019: \u20181.48\u2019, \u2018ask\u2019: \u20181.50\u2019, \u2018volume\u2019: \u20184,646\u2019, \u2018iv\u2019: \u20188.94%\u2019}{\u2018contract\u2019: \u2018SPY190417C00290000\u2019, \u2018last_trade\u2019: \u20182019\u201304\u201315\u2019, \u2018strike\u2019: \u2018290.00\u2019, \u2018last\u2019: \u20180.80\u2019, \u2018bid\u2019: \u20180.82\u2019, \u2018ask\u2019: \u20180.83\u2019, \u2018volume\u2019: \u201838,491\u2019, \u2018iv\u2019: \u20188.06%\u2019}{\u2018contract\u2019: \u2018SPY190417P00290000\u2019, \u2018last_trade\u2019: \u20182019\u201304\u201315\u2019, \u2018strike\u2019: \u2018290.00\u2019, \u2018last\u2019: \u20180.77\u2019, \u2018bid\u2019: \u20180.75\u2019, \u2018ask\u2019: \u20180.78\u2019, \u2018volume\u2019: \u201811,310\u2019, \u2018iv\u2019: \u20187.30%\u2019}{\u2018contract\u2019: \u2018SPY190417P00289000\u2019, \u2018last_trade\u2019: \u20182019\u201304\u201315\u2019, \u2018strike\u2019: \u2018289.00\u2019, \u2018last\u2019: \u20180.41\u2019, \u2018bid\u2019: \u20180.40\u2019, \u2018ask\u2019: \u20180.42\u2019, \u2018volume\u2019: \u201844,319\u2019, \u2018iv\u2019: \u20187.79%\u2019}</code></pre><h3 id=\"setting-up-recurring-data-collection\">Setting up recurring data collection</h3><p>Yahoo, by default, only returns the options for the date you specify. It\u2019s this part of the URL: <a href=\"https://finance.yahoo.com/quote/SPY/options?date=1555459200\" rel=\"noopener\">https://finance.yahoo.com/quote/SPY/options?date=<strong>1555459200</strong></a></p><p>This is a Unix timestamp, so we\u2019ll need to generate or scrape one, rather than hardcoding it in our program.</p><p>Add some dependencies:</p><pre><code>import datetime, time</code></pre><p>Let\u2019s write a quick script to generate and verify a Unix timestamp for our next set of options:</p><pre><code>def get_datestamp():  options_url = \u201chttps://finance.yahoo.com/quote/SPY/options?date=\"  today = int(time.time())  # print(today)  date = datetime.datetime.fromtimestamp(today)  yy = date.year  mm = date.month  dd = date.day</code></pre><p>The above code holds the base URL of the page we are scraping and generates a <code>datetime.date</code> object for us to use in the future.</p><p>Let\u2019s increment this date by one day, so we don\u2019t get options that have already expired.</p><pre><code>dd += 1</code></pre><p>Now, we need to convert it back into a Unix timestamp and make sure it\u2019s a valid date for options contracts:</p><pre><code> options_day = datetime.date(yy, mm, dd) datestamp = int(time.mktime(options_day.timetuple())) # print(datestamp) # print(datetime.datetime.fromtimestamp(options_stamp))</code></pre><pre><code> # vet timestamp, then return if valid for i in range(0, 7):   test_req = requests.get(options_url + str(datestamp)).content   content = BeautifulSoup(test_req, \u201chtml.parser\u201d)   # print(content)   tables = content.find_all(\u201ctable\u201d)</code></pre><pre><code> if tables != []:   # print(datestamp)   return str(datestamp) else:   # print(\u201cBad datestamp!\u201d)   dd += 1   options_day = datetime.date(yy, mm, dd)   datestamp = int(time.mktime(options_day.timetuple()))  return str(-1)</code></pre><p>Let\u2019s adapt our <code>fetch_options</code> method to use a dynamic timestamp to fetch options data, rather than whatever Yahoo wants to give us as the default.</p><p>Change this line:</p><pre><code>data_url = \u201chttps://finance.yahoo.com/quote/SPY/options\"</code></pre><p>To this:</p><pre><code>datestamp = get_datestamp()data_url = \u201chttps://finance.yahoo.com/quote/SPY/options?date=\" + datestamp</code></pre><p>Congratulations! You just scraped real-world options data from the web.</p><p>Now we need to do some simple file I/O and set up a timer to record this data each day after market close.</p><h3 id=\"improving-the-program\">Improving the program</h3><p>Rename <code>main()</code> to <code>fetch_options()</code> and add these lines to the bottom:</p><pre><code>options_list = {\u2018calls\u2019: {\u2018itm\u2019: itm_call_info, \u2018otm\u2019: otm_call_info}, \u2018puts\u2019: {\u2018itm\u2019: itm_put_info, \u2018otm\u2019: otm_put_info}, \u2018date\u2019: datetime.date.fromtimestamp(time.time()).strftime(\u201c%Y-%m-%d\u201d)}return options_list</code></pre><p>Create a new method called <code>schedule()</code>. We\u2019ll use this to control when we scrape for options, every twenty-four hours after market close. Add this code to schedule our first job at the next market close:</p><pre><code>from apscheduler.schedulers.background import BackgroundScheduler</code></pre><pre><code>scheduler = BackgroundScheduler()</code></pre><pre><code>def schedule():  scheduler.add_job(func=run, trigger=\u201ddate\u201d, run_date = datetime.datetime.now())  scheduler.start()</code></pre><p>In your <code>if __name__ == \u201c__main__\u201d:</code> statement, delete <code>main()</code> and add a call to <code>schedule()</code> to set up your first scheduled job.</p><p>Create another method called <code>run()</code>. This is where we\u2019ll handle the bulk of our operations, including actually saving the market data. Add this to the body of <code>run()</code>:</p><pre><code>  today = int(time.time()) date = datetime.datetime.fromtimestamp(today) yy = date.year mm = date.month dd = date.day</code></pre><pre><code> # must use 12:30 for Unix time instead of 4:30 NY time next_close = datetime.datetime(yy, mm, dd, 12, 30)</code></pre><pre><code> # do operations here \u201c\u201d\u201d This is where we\u2019ll write our last bit of code. \u201c\u201d\u201d</code></pre><pre><code> # schedule next job scheduler.add_job(func=run, trigger=\u201ddate\u201d, run_date = next_close)</code></pre><pre><code> print(\u201cJob scheduled! | \u201c + str(next_close))</code></pre><p>This lets our code call itself in the future, so we can just put it on a server and build up our options data each day. Add this code to actually fetch data under <code>\u201c\u201d\u201d This is where we\u2019ll write our last bit of code. \u201c\u201d\u201d</code></p><pre><code>options = {}</code></pre><pre><code> # ensures option data doesn\u2019t break the program if internet is out try:   if next_close &gt; datetime.datetime.now():     print(\u201cMarket is still open! Waiting until after close\u2026\u201d)   else:     # ensures program was run after market hours     if next_close &lt; datetime.datetime.now():      dd += 1       next_close = datetime.datetime(yy, mm, dd, 12, 30)       options = fetch_options()       print(options)       # write to file       write_to_csv(options)except:  print(\u201cCheck your connection and try again.\u201d)</code></pre><h3 id=\"saving-data\">Saving data</h3><p>You may have noticed that <code>write_to_csv</code> isn\u2019t implemented yet. No worries \u2014 let\u2019s take care of that here:</p><pre><code>def write_to_csv(options_data):  import csv  with open(\u2018options.csv\u2019, \u2018a\u2019, newline=\u2019\\n\u2019) as csvfile:  spamwriter = csv.writer(csvfile, delimiter=\u2019,\u2019)  spamwriter.writerow([str(options_data)])</code></pre><h3 id=\"cleaning-up\"><strong>Cleaning up</strong></h3><p>As options contracts are time-sensitive, we might want to add a field for their expiration date. This capability is not included in the raw HTML we scraped.</p><p>Add this line of code to save and format the expiration date towards the top of <code>fetch_options()</code>:</p><pre><code>expiration =  datetime.datetime.fromtimestamp(int(get_datestamp())).strftime(\"%Y-%m-%d\")</code></pre><p>Add <code>\u2018expiration\u2019: expiration</code> to the end of each <code>option_info</code> dictionary like so:</p><pre><code>itm_call_info = {'contract': itm_call_data[0],  'strike': itm_call_data[2], 'last': itm_call_data[3],   'bid': itm_call_data[4], 'ask': itm_call_data[5], 'volume': itm_call_data[8], 'iv': itm_call_data[10], 'expiration': expiration}</code></pre><p>Give your new program a run \u2014 it\u2019ll scrape the latest options data and write it to a .csv file as a string representation of a dictionary. The .csv file will be ready to be parsed by a backtesting program or served to users through a webapp. Congratulations!</p>\n</section>\n<hr/>\n<p>\n        Learn to code for free. freeCodeCamp's open source curriculum has helped more than 40,000 people get jobs as developers. <a href=\"https://www.freecodecamp.org/learn/\" id=\"learn-to-code-cta\" rel=\"noopener noreferrer\" target=\"_blank\">Get started</a>\n</p>\n</section>\n</article>\n</div></div>", "textContent": "\n            \n                \n                \n                \n                    \n                \n                \n                    \n                            \nby Harry SauersAn introduction to web scraping for financeEver wished you could access historical options data, but got blocked by a paywall? What if you just want it for research, fun, or to develop a personal trading strategy?In this tutorial, you\u2019ll learn how to use Python and BeautifulSoup to scrape financial data from the Web and build your own dataset.Getting StartedYou should have at least a working knowledge of Python and Web technologies before beginning this tutorial. To build these up, I highly recommend checking out a site like codecademy to learn new skills or brush up on old ones.First, let\u2019s spin up your favorite IDE. Normally, I use PyCharm but, for a quick script like this Repl.it will do the job too. Add a quick print (\u201cHello world\u201d) to ensure your environment is set up correctly.Now we need to figure out a data source.Unfortunately, Cboe\u2019s awesome options chain data is pretty locked down, even for current delayed quotes. Luckily, Yahoo Finance has solid enough options data here. We\u2019ll use it for this tutorial, as web scrapers often need some content awareness, but it is easily adaptable for any data source you want.DependenciesWe don\u2019t need many external dependencies. We just need the Requests and BeautifulSoup modules in Python. Add these at the top of your program:from bs4 import BeautifulSoupimport requestsCreate a main method:def main():  print(\u201cHello World!\u201d)if __name__ == \u201c__main__\u201d:  main()Scraping HTMLNow you\u2019re ready to start scraping! Inside main(), add these lines to fetch the page\u2019s full HTML:data_url = \u201chttps://finance.yahoo.com/quote/SPY/options\"data_html = requests.get(data_url).contentprint(data_html)This fetches the page\u2019s full HTML content, so we can find the data we want in it. Feel free to give it a run and observe the output.Feel free to comment out print statements as you go \u2014 these are just there to help you understand what the program is doing at any given step.BeautifulSoup is the perfect tool for working with HTML data in Python. Let\u2019s narrow down the HTML to just the options pricing tables so we can better understand it: content = BeautifulSoup(data_html, \u201chtml.parser\u201d) # print(content) options_tables = content.find_all(\u201ctable\u201d) print(options_tables)That\u2019s still quite a bit of HTML \u2014 we can\u2019t get much out of that, and Yahoo\u2019s code isn\u2019t the most friendly to web scrapers. Let\u2019s break it down into two tables, for calls and puts: options_tables = [] tables = content.find_all(\u201ctable\u201d) for i in range(0, len(content.find_all(\u201ctable\u201d))):   options_tables.append(tables[i]) print(options_tables)Yahoo\u2019s data contains options that are pretty deep in- and out-of-the-money, which might be great for certain purposes. I\u2019m only interested in near-the-money options, namely the two calls and two puts closest to the current price.Let\u2019s find these, using BeautifulSoup and Yahoo\u2019s differential table entries for in-the-money and out-of-the-money options:expiration = datetime.datetime.fromtimestamp(int(datestamp)).strftime(\u201c%Y-%m-%d\u201d)calls = options_tables[0].find_all(\u201ctr\u201d)[1:] # first row is headeritm_calls = []otm_calls = []for call_option in calls:    if \u201cin-the-money\u201d in str(call_option):  itm_calls.append(call_option)  else:    otm_calls.append(call_option)itm_call = itm_calls[-1]otm_call = otm_calls[0]print(str(itm_call) + \u201c \\n\\n \u201c + str(otm_call))Now, we have the table entries for the two options nearest to the money in HTML. Let\u2019s scrape the pricing data, volume, and implied volatility from the first call option: itm_call_data = [] for td in BeautifulSoup(str(itm_call), \u201chtml.parser\u201d).find_all(\u201ctd\u201d):   itm_call_data.append(td.text)print(itm_call_data)itm_call_info = {\u2018contract\u2019: itm_call_data[0], \u2018strike\u2019: itm_call_data[2], \u2018last\u2019: itm_call_data[3],  \u2018bid\u2019: itm_call_data[4], \u2018ask\u2019: itm_call_data[5], \u2018volume\u2019: itm_call_data[8], \u2018iv\u2019: itm_call_data[10]}print(itm_call_info)Adapt this code for the next call option:# otm callotm_call_data = []for td in BeautifulSoup(str(otm_call), \u201chtml.parser\u201d).find_all(\u201ctd\u201d):  otm_call_data.append(td.text)# print(otm_call_data)otm_call_info = {\u2018contract\u2019: otm_call_data[0], \u2018strike\u2019: otm_call_data[2], \u2018last\u2019: otm_call_data[3],  \u2018bid\u2019: otm_call_data[4], \u2018ask\u2019: otm_call_data[5], \u2018volume\u2019: otm_call_data[8], \u2018iv\u2019: otm_call_data[10]}print(otm_call_info)Give your program a run!You now have dictionaries of the two near-the-money call options. It\u2019s enough just to scrape the table of put options for this same data:puts = options_tables[1].find_all(\"tr\")[1:]  # first row is headeritm_puts = []  otm_puts = []for put_option in puts:    if \"in-the-money\" in str(put_option):      itm_puts.append(put_option)    else:      otm_puts.append(put_option)itm_put = itm_puts[0]  otm_put = otm_puts[-1]# print(str(itm_put) + \" \\n\\n \" + str(otm_put) + \"\\n\\n\")itm_put_data = []  for td in BeautifulSoup(str(itm_put), \"html.parser\").find_all(\"td\"):    itm_put_data.append(td.text)# print(itm_put_data)itm_put_info = {'contract': itm_put_data[0],                  'last_trade': itm_put_data[1][:10],                  'strike': itm_put_data[2], 'last': itm_put_data[3],                   'bid': itm_put_data[4], 'ask': itm_put_data[5], 'volume': itm_put_data[8], 'iv': itm_put_data[10]}# print(itm_put_info)# otm put  otm_put_data = []  for td in BeautifulSoup(str(otm_put), \"html.parser\").find_all(\"td\"):    otm_put_data.append(td.text)# print(otm_put_data)otm_put_info = {'contract': otm_put_data[0],                  'last_trade': otm_put_data[1][:10],                  'strike': otm_put_data[2], 'last': otm_put_data[3],                   'bid': otm_put_data[4], 'ask': otm_put_data[5], 'volume': otm_put_data[8], 'iv': otm_put_data[10]}Congratulations! You just scraped data for all near-the-money options of the S&P 500 ETF, and can view them like this: print(\"\\n\\n\") print(itm_call_info) print(otm_call_info) print(itm_put_info) print(otm_put_info)Give your program a run \u2014 you should get data like this printed to the console:{\u2018contract\u2019: \u2018SPY190417C00289000\u2019, \u2018last_trade\u2019: \u20182019\u201304\u201315\u2019, \u2018strike\u2019: \u2018289.00\u2019, \u2018last\u2019: \u20181.46\u2019, \u2018bid\u2019: \u20181.48\u2019, \u2018ask\u2019: \u20181.50\u2019, \u2018volume\u2019: \u20184,646\u2019, \u2018iv\u2019: \u20188.94%\u2019}{\u2018contract\u2019: \u2018SPY190417C00290000\u2019, \u2018last_trade\u2019: \u20182019\u201304\u201315\u2019, \u2018strike\u2019: \u2018290.00\u2019, \u2018last\u2019: \u20180.80\u2019, \u2018bid\u2019: \u20180.82\u2019, \u2018ask\u2019: \u20180.83\u2019, \u2018volume\u2019: \u201838,491\u2019, \u2018iv\u2019: \u20188.06%\u2019}{\u2018contract\u2019: \u2018SPY190417P00290000\u2019, \u2018last_trade\u2019: \u20182019\u201304\u201315\u2019, \u2018strike\u2019: \u2018290.00\u2019, \u2018last\u2019: \u20180.77\u2019, \u2018bid\u2019: \u20180.75\u2019, \u2018ask\u2019: \u20180.78\u2019, \u2018volume\u2019: \u201811,310\u2019, \u2018iv\u2019: \u20187.30%\u2019}{\u2018contract\u2019: \u2018SPY190417P00289000\u2019, \u2018last_trade\u2019: \u20182019\u201304\u201315\u2019, \u2018strike\u2019: \u2018289.00\u2019, \u2018last\u2019: \u20180.41\u2019, \u2018bid\u2019: \u20180.40\u2019, \u2018ask\u2019: \u20180.42\u2019, \u2018volume\u2019: \u201844,319\u2019, \u2018iv\u2019: \u20187.79%\u2019}Setting up recurring data collectionYahoo, by default, only returns the options for the date you specify. It\u2019s this part of the URL: https://finance.yahoo.com/quote/SPY/options?date=1555459200This is a Unix timestamp, so we\u2019ll need to generate or scrape one, rather than hardcoding it in our program.Add some dependencies:import datetime, timeLet\u2019s write a quick script to generate and verify a Unix timestamp for our next set of options:def get_datestamp():  options_url = \u201chttps://finance.yahoo.com/quote/SPY/options?date=\"  today = int(time.time())  # print(today)  date = datetime.datetime.fromtimestamp(today)  yy = date.year  mm = date.month  dd = date.dayThe above code holds the base URL of the page we are scraping and generates a datetime.date object for us to use in the future.Let\u2019s increment this date by one day, so we don\u2019t get options that have already expired.dd += 1Now, we need to convert it back into a Unix timestamp and make sure it\u2019s a valid date for options contracts: options_day = datetime.date(yy, mm, dd) datestamp = int(time.mktime(options_day.timetuple())) # print(datestamp) # print(datetime.datetime.fromtimestamp(options_stamp)) # vet timestamp, then return if valid for i in range(0, 7):   test_req = requests.get(options_url + str(datestamp)).content   content = BeautifulSoup(test_req, \u201chtml.parser\u201d)   # print(content)   tables = content.find_all(\u201ctable\u201d) if tables != []:   # print(datestamp)   return str(datestamp) else:   # print(\u201cBad datestamp!\u201d)   dd += 1   options_day = datetime.date(yy, mm, dd)   datestamp = int(time.mktime(options_day.timetuple()))  return str(-1)Let\u2019s adapt our fetch_options method to use a dynamic timestamp to fetch options data, rather than whatever Yahoo wants to give us as the default.Change this line:data_url = \u201chttps://finance.yahoo.com/quote/SPY/options\"To this:datestamp = get_datestamp()data_url = \u201chttps://finance.yahoo.com/quote/SPY/options?date=\" + datestampCongratulations! You just scraped real-world options data from the web.Now we need to do some simple file I/O and set up a timer to record this data each day after market close.Improving the programRename main() to fetch_options() and add these lines to the bottom:options_list = {\u2018calls\u2019: {\u2018itm\u2019: itm_call_info, \u2018otm\u2019: otm_call_info}, \u2018puts\u2019: {\u2018itm\u2019: itm_put_info, \u2018otm\u2019: otm_put_info}, \u2018date\u2019: datetime.date.fromtimestamp(time.time()).strftime(\u201c%Y-%m-%d\u201d)}return options_listCreate a new method called schedule(). We\u2019ll use this to control when we scrape for options, every twenty-four hours after market close. Add this code to schedule our first job at the next market close:from apscheduler.schedulers.background import BackgroundSchedulerscheduler = BackgroundScheduler()def schedule():  scheduler.add_job(func=run, trigger=\u201ddate\u201d, run_date = datetime.datetime.now())  scheduler.start()In your if __name__ == \u201c__main__\u201d: statement, delete main() and add a call to schedule() to set up your first scheduled job.Create another method called run(). This is where we\u2019ll handle the bulk of our operations, including actually saving the market data. Add this to the body of run():  today = int(time.time()) date = datetime.datetime.fromtimestamp(today) yy = date.year mm = date.month dd = date.day # must use 12:30 for Unix time instead of 4:30 NY time next_close = datetime.datetime(yy, mm, dd, 12, 30) # do operations here \u201c\u201d\u201d This is where we\u2019ll write our last bit of code. \u201c\u201d\u201d # schedule next job scheduler.add_job(func=run, trigger=\u201ddate\u201d, run_date = next_close) print(\u201cJob scheduled! | \u201c + str(next_close))This lets our code call itself in the future, so we can just put it on a server and build up our options data each day. Add this code to actually fetch data under \u201c\u201d\u201d This is where we\u2019ll write our last bit of code. \u201c\u201d\u201doptions = {} # ensures option data doesn\u2019t break the program if internet is out try:   if next_close > datetime.datetime.now():     print(\u201cMarket is still open! Waiting until after close\u2026\u201d)   else:     # ensures program was run after market hours     if next_close < datetime.datetime.now():      dd += 1       next_close = datetime.datetime(yy, mm, dd, 12, 30)       options = fetch_options()       print(options)       # write to file       write_to_csv(options)except:  print(\u201cCheck your connection and try again.\u201d)Saving dataYou may have noticed that write_to_csv isn\u2019t implemented yet. No worries \u2014 let\u2019s take care of that here:def write_to_csv(options_data):  import csv  with open(\u2018options.csv\u2019, \u2018a\u2019, newline=\u2019\\n\u2019) as csvfile:  spamwriter = csv.writer(csvfile, delimiter=\u2019,\u2019)  spamwriter.writerow([str(options_data)])Cleaning upAs options contracts are time-sensitive, we might want to add a field for their expiration date. This capability is not included in the raw HTML we scraped.Add this line of code to save and format the expiration date towards the top of fetch_options():expiration =  datetime.datetime.fromtimestamp(int(get_datestamp())).strftime(\"%Y-%m-%d\")Add \u2018expiration\u2019: expiration to the end of each option_info dictionary like so:itm_call_info = {'contract': itm_call_data[0],  'strike': itm_call_data[2], 'last': itm_call_data[3],   'bid': itm_call_data[4], 'ask': itm_call_data[5], 'volume': itm_call_data[8], 'iv': itm_call_data[10], 'expiration': expiration}Give your new program a run \u2014 it\u2019ll scrape the latest options data and write it to a .csv file as a string representation of a dictionary. The .csv file will be ready to be parsed by a backtesting program or served to users through a webapp. Congratulations!\n\n                        \n                    \n                    \n\n                    \n                    \n                        \n    \n\n\n\n\n\n    \n    \n\n\n                        \n\n\n        Learn to code for free. freeCodeCamp's open source curriculum has helped more than 40,000 people get jobs as developers. Get started\n    \n\n                    \n                \n                \n                    \n                \n            \n        ", "length": 12735, "excerpt": "by Harry Sauers\n\nHow I get options data for free\nAn introduction to web scraping for finance\nEver wished you could access historical options data, but got blocked by a\npaywall? What if you just want it for research, fun, or to develop a personal\ntrading strategy?\n\nIn this tutorial, you\u2019ll learn how to use Python and BeautifulSoup to scrape\nfinancial data from the Web and build your own dataset.\n\nGetting Started\nYou should have at least a working knowledge of Python and Web technologies\nbefore be", "siteName": "freeCodeCamp.org", "publishedTime": "2019-05-10T17:05:11.000Z", "id": "bdb60a7632d0348753a01178c8ebde8f0c16774e", "url": "https://www.freecodecamp.org/news/how-i-get-options-data-for-free-fba22d395cc8/", "domain": "freecodecamp.org", "date": "2023-12-27T18:10:07.165167", "resultUri": "http://localhost:3000/result/bdb60a7632d0348753a01178c8ebde8f0c16774e", "query": {"url": ["https://www.freecodecamp.org/news/how-i-get-options-data-for-free-fba22d395cc8/"]}, "meta": {"og": {"site_name": "freeCodeCamp.org", "type": "article", "title": "How I get options data for free", "description": "by Harry Sauers How I get options data for free An introduction to web scraping for finance Ever wished you could access historical options data, but got blocked by a paywall? What if you just want it for research, fun, or to develop a personal trading strategy? In this tutorial,", "url": "https://www.freecodecamp.org/news/how-i-get-options-data-for-free-fba22d395cc8/", "image": "https://cdn-media-1.freecodecamp.org/images/0*8B9koPyETFCwVvO6.png", "image:width": "560", "image:height": "478"}, "twitter": {"card": "summary_large_image", "title": "How I get options data for free", "description": "by Harry Sauers How I get options data for free An introduction to web scraping for finance Ever wished you could access historical options data, but got blocked by a paywall? What if you just want it for research, fun, or to develop a personal trading strategy? In this tutorial,", "url": "https://www.freecodecamp.org/news/how-i-get-options-data-for-free-fba22d395cc8/", "image": "https://cdn-media-1.freecodecamp.org/images/0*8B9koPyETFCwVvO6.png", "label1": "Written by", "data1": "freeCodeCamp.org", "label2": "Filed under", "data2": "Python, Finance, Web Scraping, Tech, Programming", "site": "@freecodecamp"}}}